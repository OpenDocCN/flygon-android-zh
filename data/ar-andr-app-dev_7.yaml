- en: Chapter 7. Further Reading and Tips
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 进一步阅读与技巧
- en: In this final chapter, we will present you with tips and links to more advanced
    techniques to improve any AR application's development. We will introduce content
    management techniques such as multi-targets and cloud recognition, as well as
    advanced interaction techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将为您提供更多高级技巧以及相关链接，以帮助提升任何AR应用的开发。我们将介绍内容管理技巧，如多目标和云识别，以及高级交互技术。
- en: Managing your content
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理你的内容
- en: For computer-vision-based AR, we showed you how to build applications using
    a single target. However, there might be scenarios in which you need to use several
    markers at once. Just think of augmenting a room for which you would need at least
    one target on each wall, or you may want your application to be able to recognize
    and augment hundreds of different product packages. The former case can be achieved
    by tracking multiple targets that have a common coordinate frame, and the latter
    use case can be achieved by using the power of cloud recognition. We will briefly
    discuss both of them in the following sections.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于计算机视觉的AR，我们向你展示了如何使用单一目标构建应用。然而，在某些情况下，你可能需要同时使用多个标记。想象一下增强一个房间，你需要在每面墙上至少有一个目标，或者你可能希望你的应用能够识别并增强数百个不同的产品包装。前者可以通过追踪具有公共坐标框架的多个目标来实现，后者可以通过使用云识别的力量来实现。我们将在以下各节简要讨论这两种情况。
- en: Multi-targets
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多目标
- en: '**Multi-targets** are more than a collection of several individual images.
    They realize a single and consistent coordinate system where a handheld device
    can be tracked. This allows for continuous augmentation of the scene as long as
    even a single target is visible. The main challenges of creating multi-targets
    lie in defining the common coordinate system (which you will do only once) and
    maintaining the relative poses of those targets during the operation of the device.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**多目标**不仅仅是几个单独图像的集合。它们实现了一个单一且连贯的坐标系统，手持设备可以在其中被追踪。只要至少有一个目标可见，这就允许场景的持续增强。创建多目标的主要挑战在于定义公共坐标系统（只需做一次）以及在设备运行期间保持这些目标的相对位置。'
- en: 'To create a common coordinate system, you have to specify the translation and
    orientation of all image targets with respect to a common origin. Vuforia^(TM)
    gives you an option to even build commonly used multi-targets such as cubes or
    cuboids without getting into the details of specifying the entire target transforms.
    In the Vuforia^(TM) Target Manager, you can simply add a cube (equal length, height,
    and width) or cuboids (different length, height, and width) to a target that has
    its coordinate origin at the (invisible) center of the cuboids. All you have to
    do is to specify one extend to three extends of the cuboids and add individual
    images for all the sides of your targets, as shown in the following figure:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建公共坐标系统，你必须指定所有图像目标相对于公共原点的平移和方向。Vuforia^(TM)为你提供了一个选项，即使不涉及指定整个目标变换的细节，也可以构建常用的多目标，如立方体或长方体。在Vuforia^(TM)目标管理器中，你可以简单地为具有立方体（长度、高度和宽度相等）或长方体（长度、高度和宽度不同）的目标添加一个立方体，该目标的坐标原点在（不可见）长方体的中心。你需要做的就是指定一个扩展到三个扩展的长方体，并为你的目标的每一面添加单独的图像，如下图所示：
- en: '![Multi-targets](img/8553OS_07_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![多目标](img/8553OS_07_01.jpg)'
- en: If you want to create more complex multi-targets, for example, for tracking
    an entire room, you have to take a slightly different approach. You first upload
    all the images you want to use for the multi-target into a single device database
    inside the Vuforia^(TM) Target Manager. After, you have downloaded the device
    database to your development machine, you can then modify the downloaded `<database
    >.xml` file to add the names of the individual image targets and their translations
    and orientations relative to the coordinate origin. A sample XML file can be found
    in the Vuforia^(TM) knowledge base under [https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file](https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要创建更复杂的多目标，例如，追踪整个房间，你必须采取略有不同的方法。首先，你需要在Vuforia^(TM)目标管理器中的单个设备数据库内上传所有想要用于多目标的多张图片。之后，将设备数据库下载到你的开发机上，然后你可以修改下载的`<database>.xml`文件，以添加单个图像目标的名字以及它们相对于坐标原点的平移和方向。你可以在Vuforia^(TM)知识库中找到示例XML文件，链接为[https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file](https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file)。
- en: Note that you can only have a maximum of 100 targets in your device database,
    and hence your multi-target can maximally consist of only that number of image
    targets. Also note that changing the position of image targets during the runtime
    (for example, opening a product packaging) will inhibit consistent tracking of
    your coordinate system, that is, the defined spatial relationships between the
    individual target elements would not be valid anymore. This can even lead to complete
    failure of tracking. If you want to use individual moving elements as part of
    your application, you have to define them in addition to the multi-target as separate
    image targets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您的设备数据库中最多只能有100个目标，因此，您的多目标最多只能由这么多图像目标组成。还要注意，在运行时更改图像目标的位置（例如，打开产品包装）将阻碍对您的坐标系的一致跟踪，即，各个目标元素之间定义的空间关系将不再有效。这甚至可能导致跟踪完全失败。如果您想将单个移动元素作为应用程序的一部分使用，除了多目标之外，您还必须将它们定义为单独的图像目标。
- en: Cloud recognition
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云端识别
- en: As mentioned in the preceding section, you can only use up to 100 images simultaneously
    in your Vuforia^(TM) application. This limitation can be overcome by using cloud
    databases. The basic idea here is that you query a cloud service with a camera
    image, and (if the target is recognized in the cloud), handle the tracking of
    the recognized target locally on your device. The major benefit of this approach
    is that you can recognize up to one million images that should be sufficient for
    most application scenarios. However, this benefit does not come for free. As the
    recognition happens in the cloud, your client has to be connected to the Internet,
    and the response time can take up to several seconds (typically around two to
    three seconds).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在您的Vuforia^(TM)应用程序中，同时只能使用多达100个图像。通过使用云数据库，可以克服这一限制。这里的基本思想是，您使用摄像头图像查询云服务，如果在云端识别到目标，则可以在本地设备上处理已识别目标的跟踪。这种方法的主要优点是，您可以识别多达一百万张图像，这对于大多数应用场景来说应该是足够的。然而，这种好处并非没有代价。由于识别过程在云端进行，您的客户端必须连接到互联网，响应时间可能需要几秒钟（通常大约两到三秒）。
- en: 'Unlike, in the case of recognition, image databases stored on the device typically
    only take about 60 to 100 milliseconds. To make it easier to upload many images
    for the cloud recognition, you do not even have to use the Vuforia^(TM) online
    target manager website but can use a specific web API—the Vuforia^(TM) Web Services
    API—that can be found under the following URL: [https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api](https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api).
    You can find further information about using cloud recognition in the Vuforia^(TM)
    knowledge base by visiting [https://developer.vuforia.com/resources/dev-guide/cloud-targets](https://developer.vuforia.com/resources/dev-guide/cloud-targets).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与云端识别不同，存储在设备上的图像数据库通常只需要大约60到100毫秒。为了更容易上传大量图像以供云端识别，您甚至不需要使用Vuforia^(TM)在线目标管理网站，而可以使用特定的Web
    API——Vuforia^(TM) Web服务API，该API可以在以下URL找到：[https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api](https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api)。您可以通过访问[https://developer.vuforia.com/resources/dev-guide/cloud-targets](https://developer.vuforia.com/resources/dev-guide/cloud-targets)在Vuforia^(TM)知识库中找到有关使用云端识别的更多信息。
- en: Improving recognition and tracking
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高识别和跟踪
- en: If you want to create your own natural feature-tracking targets, it is important
    to design them in a way that they can be well recognized and tracked by the AR
    system. The basics of natural feature targets were explained in the *Understanding
    natural feature tracking targets* section of [Chapter 5](ch05.html "Chapter 5. Same
    as Hollywood – Virtual on Physical Objects"), *Same as Hollywood – Virtual on
    Physical Objects*. The basic requirement for well-traceable targets is that they
    possess a high number of local features. But how do you go along if your target
    is not well recognized? To a certain extent, you can improve the tracking by using
    the forthcoming tips.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想创建自己的自然特征跟踪目标，重要的是要设计它们，以便AR系统能够很好地识别和跟踪。第5章《与好莱坞相同——虚拟物体上的物理对象》中的*了解自然特征跟踪目标*部分介绍了自然特征目标的基础。可良好追踪的目标的基本要求是它们具有大量的局部特征。但如果您的目标识别不佳怎么办？在一定程度上，您可以通过使用以下技巧来改善跟踪。
- en: 'First, you want to make sure that your images have enough local contrast. A
    good indicator for the overall contrast in your target is to have a look at the
    histogram of its greyscale representation in any photo editing software such as
    GIMP or Photoshop. You generally want a widely distributed histogram instead of
    one with few spikes, as shown in the following figure:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你想要确保图像具有足够的局部对比度。在GIMP或Photoshop等任何照片编辑软件中查看目标灰度表示的直方图是判断目标整体对比度的一个好方法。你通常希望直方图分布广泛，而不是像下面图中那样只有少数尖峰：
- en: '![Improving recognition and tracking](img/8553OS_07_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![提高识别和追踪](img/8553OS_07_02.jpg)'
- en: To increase the local contrast in your images, you can use the photo editor
    of your choice and apply unsharpening mask filters or clarity filters, such as
    in Adobe Lightroom.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要增加图像的局部对比度，你可以使用你选择的照片编辑器，并应用去锐化蒙版滤镜或清晰度滤镜，如在Adobe Lightroom中。
- en: Tip
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In addition, to avoid resampling artifacts in the Vuforia^(TM) target creation
    process, make sure to upload your individual images with an exact image width
    of 320 px. This will avoid aliasing effects and lowering the local feature count
    due to automatic server-side resizing of your images. By improving the rendering,
    Vuforia^(TM) will rescale your images to have a maximum extend of 320 px for the
    longest image side.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在Vuforia^(TM)目标创建过程中出现重采样伪影，请确保上传的单个图像具有精确的320像素图像宽度。这样可以避免因服务器端自动调整图像大小而导致的走样效果和局部特征数量减少。通过改善渲染效果，Vuforia^(TM)会将你的图像重新缩放到最长图像边最大为320像素。
- en: During the course of this book, we used different types of 3D models in our
    sample applications, including basic primitives (such as our colored cube or sphere)
    or more advanced 3D models (such as the ninja model). For all of them, we didn't
    really consider the realistic aspect, including the light condition. Any desktop
    or mobile 3D application will always consider how the rendering looks realistic.
    This photorealistic quest always passes through the quality of the geometry of
    the model, the definition of their appearance (material reflectance properties),
    and how they interact with light (shading and illumination).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的示例应用中，我们使用了不同类型的3D模型，包括基本图元（如我们的彩色立方体或球体）或更高级的3D模型（如忍者模型）。对于所有这些模型，我们并没有真正考虑真实感，包括光线条件。任何桌面或移动3D应用程序都会考虑渲染看起来如何真实。这种真实感追求始终通过模型几何质量、外观定义（材质反射属性）以及它们如何与光线交互（着色和照明）来体现。
- en: '**Photorealistic rendering** will expose properties such as occlusion (what
    is in front of, behind something), shadows (from the illumination), support for
    a range of realistic material (developed with shader technology), or more advanced
    properties such as supporting global illumination.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实感渲染**将展示诸如遮挡（某物的前后）阴影（来自光照）对一系列真实材质的支持（使用着色器技术开发）或更高级的属性，例如支持全局照明。'
- en: 'When you develop your AR applications, you should also consider photorealistic
    rendering. However, things are a bit more complicated because in AR, you not only
    consider the virtual aspect (for example, a desktop 3D game) but also the real
    aspect. Supporting photorealism in AR will imply that you consider **how real
    (R**) **environments and virtual** (**V**) **environments** also interact during
    the rendering that can be simplified as follows through four different cases:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开发AR应用时，还应该考虑真实感渲染。然而，事情会有些复杂，因为在AR中，你不仅要考虑虚拟方面（例如，桌面3D游戏），还要考虑真实方面。在AR中支持真实感意味着你需要考虑**真实（R）环境和虚拟（V）环境**在渲染过程中如何交互，可以通过以下四种不同情况简化如下：
- en: V→V
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: V→V
- en: V→R
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: V→R
- en: R→V
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R→V
- en: R→R
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R→R
- en: The easiest thing you can do is support V→V, which means that you enable any
    of the advanced rendering techniques in your 3D rendering engine. For computer-vision-based
    applications, it will mean that everything looks realistic on your target. For
    sensor-based applications, it will mean that your virtual object seems realistic
    between each other.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的事情是支持V→V，这意味着你可以在3D渲染引擎中启用任何高级渲染技术。对于基于计算机视觉的应用程序，这意味着目标上的所有东西看起来都是真实的。对于基于传感器的应用程序，这意味着你的虚拟对象在彼此之间看起来是真实的。
- en: 'A second easy step, especially for computer-vision-based applications, is to
    support V→R using a plane technique. If you have a target, you can create a semi-transparent
    version of it and add it to your virtual scene. If you have shadows enabled, it
    will seem that the shadow is projecting on to your target, creating a simple illusion
    of V→R. You can refer to the following paper which will provide you with some
    technical solutions to this problem:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于计算机视觉的应用程序，第二步是使用平面技术支持V→R（虚拟到现实）。如果你有一个目标，可以创建一个半透明的版本并将其添加到你的虚拟场景中。如果你启用了阴影，那么看起来阴影会投射到你的目标上，从而创建一个简单的V→R（虚拟到现实）的错觉。你可以参考以下论文，其中提供了一些解决此问题的技术方案：
- en: 'Refer to *A real-time shadow approach for an augmented reality application
    using shadow volumes. VRST 2003: 56-65* by *Michael Haller*, *Stephan Drab*, and
    *Werner Hartmann*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '请参考*Michael Haller*、*Stephan Drab*和*Werner Hartmann*所著的《一种用于增强现实应用的实时阴影方法。VRST
    2003: 56-65》。'
- en: Handling R→V is a bit more complicated and still a difficult research topic.
    For example, support illumination of virtual objects by physical light sources
    requires a lot of effort.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 处理R→V（现实到虚拟）要复杂一些，仍然是一个困难的研究课题。例如，支持虚拟对象由物理光源照明需要付出大量的努力。
- en: Instead, occlusion is easy to implement for R→V. Occlusion in the case of R→V
    can happen if, for example, a physical object (such as a **can**) is placed in
    front of your virtual object. In standard AR, you always render the virtual content
    in front of the video, so your **can** will appear to be behind even though it
    can be in front of your target.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，对于R→V来说，遮挡是容易实现的。如果例如一个物理对象（如一个**罐头**）放在你的虚拟对象前面，那么R→V情况下的遮挡就会发生。在标准AR（增强现实）中，你总是将虚拟内容渲染在视频前面，所以即使你的**罐头**实际上在目标前面，它看起来也会在后面。
- en: A simple technique to reproduce this effect is sometimes referred to as **phantom
    object**. You need to create a virtual counterpart of your physical object, such
    as a cylinder, to represent your can. Place this virtual counterpart at the same
    position as the physical one and do a **depth-only rendering**. Depth-only rendering
    is available in a large range of libraries, and it's related to the color mask
    where, when you render anything, you can decide which channel to render. Commonly,
    you have the combination of red, green, blue, and depth. So, you need to deactivate
    the first three channels and only activate depth. It will render some sort of
    phantom object (no color but only depth), and via the standard rendering pipeline,
    the video will not be occluded anymore where you have your real object, and occlusion
    will look realistic; see, for example, [http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf](http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 产生这种效果的一个简单技术有时被称为**幻影对象**。你需要创建一个物理对象的虚拟对应物，比如一个圆柱体来代表你的罐头。将这个虚拟对应物放置在与物理对象相同的位置，并进行**仅深度渲染**。仅深度渲染在许多库中都可以使用，它与颜色遮罩有关，当你渲染任何东西时，你可以决定要渲染哪个通道。通常，你有红色、绿色、蓝色和深度的组合。因此，你需要关闭前三个通道，只激活深度。它将渲染一种幻影对象（没有颜色，只有深度），通过标准的渲染管线，在你有真实对象的地方，视频将不再被遮挡，遮挡看起来会非常真实；例如，请参阅[http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf](http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf)。
- en: This is the simple case; when you have a dynamic object, things are way more
    complicated, and you need to be able to track your objects, to update their phantom
    models, and to be able to get a photorealistic rendering.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种简单的情况；当你有一个动态对象时，事情要复杂得多，你需要能够跟踪你的对象，更新它们的幻影模型，并获得逼真的渲染效果。
- en: Advanced interaction techniques
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级交互技术
- en: In the preceding chapter, we looked at some simple interaction techniques, that
    included ray picking (via touch interaction), sensor interaction, or camera to
    target proximity. There are a large number of other interaction techniques that
    can be used in Augmented Reality.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们研究了一些简单的交互技术，包括光线选择（通过触摸交互）、传感器交互，或者摄像头与目标的接近性。在增强现实中，还有大量其他的交互技术可以使用。
- en: One standard technique that we will also find on other mobile user interfaces,
    is a **virtual control pad**. As a mobile phone limits access to additional control
    devices, such as a joypad or joystick, you can emulate their behavior via a touch
    interface. With this technique, you can display a virtual controller on your screen
    and analyze the touch in this area as being equivalent to controlling a control
    pad. It's easy to implement and enhance the basic ray-casting technique. Control
    pads are generally displayed near the border of the screen, adapting to the form
    factor and grasping the gesture you make when you hold the device, so you can
    hold the device with your hand and naturally move your finger on the screen.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在其他移动用户界面也会发现的一种标准技术是**虚拟控制面板**。由于移动电话限制了访问额外的控制设备，比如游戏手柄或操纵杆，你可以通过触摸界面模拟它们的行为。使用这项技术，你可以在屏幕上显示一个虚拟控制器，并将这一区域的触摸分析等同于控制一个控制面板。它易于实现并增强基本的射线投射技术。控制面板通常显示在屏幕边缘附近，适应形态因素，并捕捉到你持握设备时的手势，这样你可以用手握住设备，并在屏幕上自然地移动手指。
- en: Another technique that is really popular in Augmented Reality is **Tangible
    User Interface** (**TUI**). When we created the sample using the concept of a
    camera to target proximity, we practically implemented a Tangible User Interface.
    The idea of a TUI is to use a physical object for supporting interaction. The
    concept was largely developed and enriched by *Iroshi Ishii* from the Tangible
    Media Group at MIT—the website to refer to is [http://tangible.media.mit.edu/](http://tangible.media.mit.edu/).
    *Mark Billinghurst* during his Ph.D. applied this concept to Augmented Reality
    and demonstrated a range of dedicated interaction techniques with it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 增强现实（AR）中另一种非常流行的技术是**有形用户界面**（**TUI**）。当我们使用摄像头对准近距离的概念来创建示例时，实际上我们实现了有形用户界面。TUI的理念是使用物理对象来支持交互。这一概念主要由麻省理工学院的Tangible
    Media Group的*Iroshi Ishii*教授大力开发和丰富——相关网站可以参考[http://tangible.media.mit.edu/](http://tangible.media.mit.edu/)。*Mark
    Billinghurst*在他的博士研究中将这一概念应用于增强现实，并展示了一系列专用的交互技术。
- en: The first type of TUI AR is **local interaction**, where you can, for example,
    use two targets for interaction. Similar to the way we detected the distance between
    the camera and target in our `ProximityBasedJME` project, you can replicate the
    same idea with two targets. You can detect whether two targets are close to each
    other, aligned in the same direction, and trigger some actions with it. You can
    use this type of interaction for card-based games when you want cards to interact
    with each other, or games that include puzzles where users need to combine different
    cards together, and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TUI AR的第一种类型是**本地交互**，例如，你可以使用两个目标进行交互。类似于我们在`ProximityBasedJME`项目中检测摄像头与目标之间的距离的方式，你可以用两个目标复制相同的想法。你可以检测两个目标是否彼此靠近，是否在同一方向上对齐，并触发一些动作。当你在卡片游戏中希望卡片之间进行交互，或者包含谜题的游戏需要用户组合不同的卡片时，你可以使用这种类型的交互。
- en: 'A second type of TUI AR is **global interaction** where you will also use two
    or more targets, but one of the targets will become *special*. What you do in
    this case is define a target as being a base target, and all the other targets
    refer to it. To implement it, you just compute the local transformation of the
    other targets to the base target, with the base target behind and defined as your
    origin. With this, it''s really easy to place targets on the main target, somehow
    defining some kind of ground plane and performing a range of different types of
    interaction with it. *Mark Billinghurst* introduced a famous derivate version
    of it, for performing paddle-based interaction. In this case, one of the targets
    is used as a paddle and can be used to interact on the ground plane—you can touch
    the ground plane, have the paddle at a specific position on the ground plane,
    or even detect a simple gesture with it (shake the paddle, tilt the paddle, and
    so on). To set up mobile AR, you need to consider the fact that end users hold
    a device and can''t perform complex gestures, but with a mobile phone, interaction
    with one hand is still possible. Refer to the following technical papers:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型的TUI AR是**全局交互**，这种交互方式也需要使用两个或更多目标，但其中一个目标会变得*特殊*。在这种情况下，你需要定义一个目标作为基础目标，其他所有目标都参照这个基础目标。实现时，你只需计算其他目标相对于基础目标的局部变换，以基础目标作为原点并在其背后定义。这样，将目标放置在主目标上就变得非常简单，某种程度上定义了一个地面平面，并且可以与它进行一系列不同类型的交互。《Mark
    Billinghurst》介绍了一个著名的衍生版本，用于执行基于拨片的交互。在这种情况下，其中一个目标被用作拨片，可以用来在地面平面上进行交互——你可以触摸地面平面，让拨片在地面平面的特定位置，甚至用它来检测简单的手势（摇动拨片、倾斜拨片等）。为了设置移动AR，你需要考虑到最终用户手持设备并不能执行复杂的手势，但使用手机时，单手交互仍然是可能的。参考以下技术论文：
- en: '*Tangible augmented reality. ACM SIGGRAPH ASIA (2008): 1-10* by *Mark Billinghurst*,
    *Hirokazu Kato*, and *Ivan Poupyrev*.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有形的增强现实。ACM SIGGRAPH ASIA（2008）：1-10*，作者*Mark Billinghurst*、*Hirokazu Kato*和*Ivan
    Poupyrev*。'
- en: '*Designing augmented reality interfaces. ACM Siggraph Computer Graphics 39.1
    (2005): 17-22* by *Mark Billinghurst*, *Raphael Grasset*, and *Julian Looser*.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*设计增强现实界面。ACM Siggraph Computer Graphics 39.1（2005）：17-22*，作者*Mark Billinghurst*、*Raphael
    Grasset*和*Julian Looser*。'
- en: 'Global interaction with a TUI, in a sense, can be defined as interaction *behind
    the screen*, while virtual control pad can be seen as interaction *in front of
    the screen*. This is another way to classify interaction with a mobile, which
    brings us to the third category of interaction techniques: **touch interaction
    on the target**. The Vuforia^(TM) library implements, for example, the concept
    of virtual buttons. A specific area on your target can be used to place the controller
    (for example, buttons, sliders, and dial), and users can place their finger on
    this area and control these elements. The concept behind this uses a time-based
    approach; if you keep your finger placed on this area for a long time, it simulates
    a click that you can have on a computer, or a tap you can do on a touch screen.
    Refer to [https://developer.vuforia.com/resources/sample-apps/virtual-button-sample-app](https://developer.vuforia.com/resources/sample-apps/virtual-button-sample-app),
    for example.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，与TUI的全局交互可以被定义为*屏幕背后*的交互，而虚拟控制面板可以被看作是*屏幕前方*的交互。这是与移动设备交互的另一种分类方式，它将我们引向了第三类交互技术：**目标上的触摸交互**。例如，Vuforia^(TM)库实现了虚拟按钮的概念。目标上的特定区域可以用来放置控制器（例如，按钮、滑块和旋钮），用户可以将手指放在这个区域上并控制这些元素。这个概念背后的原理是基于时间的；如果你将手指长时间放在这个区域，它会模拟你在电脑上可能进行的点击，或者在触摸屏上进行的轻敲。[例如，参考](https://developer.vuforia.com/resources/sample-apps/virtual-button-sample-app)。
- en: There are other techniques that are investigated in research laboratories, and
    they will soon become available to the future generation of mobile AR, so you
    should already think about them also when will be available. One trend is towards
    3D gesture interaction or also called **mid-air interaction**. Rather than touching
    your screen or touching your target, you can imagine making gestures between the
    device and the target. Having a mobile AR for 3D modeling would be an appropriate
    technique. 3D gestures have a lot of challenges such as recognizing the hand,
    the fingers, the gesture, physical engagement that can result in fatigue, and
    so on. In the near future, this type of interaction, which is already popular
    on smart home devices (such as Microsoft Kinect), will be available on devices
    (equipped with 3D sensors).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 研究实验室正在探索其他技术，这些技术很快就会成为未来移动增强现实（AR）的可用技术，因此在你考虑它们何时可用时，也应该将它们考虑在内。一个趋势是朝向3D手势交互，也被称为**空中交互**。你不再需要触摸屏幕或目标，可以想象在设备和目标之间进行手势操作。对于3D建模来说，移动AR将是一种适当的技巧。3D手势面临许多挑战，比如识别手、手指、手势，以及可能导致疲劳的身体参与等等。在不久的将来，这种已经在智能家居设备（如微软的Kinect）上普及的交互方式，将配备3D传感器的设备上得到应用。
- en: Summary
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we showed you how to go beyond the standard AR applications
    by using multi-targets or cloud recognition for computer-vision-based AR. We also
    showed you how you can improve the tracking performance for your image targets.
    In addition, we introduced you to some advanced rendering techniques for your
    AR applications. Finally, we also showed you some novel interaction techniques
    that you can use to create great AR experiences. This chapter concludes your introduction
    to the world of Augmented Reality development for Android. We hope you are ready
    to progress onto new levels of AR application development.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您展示了如何通过使用多目标或云识别来超越标准的AR应用，基于计算机视觉的AR。我们还向您展示了如何提高图像目标的跟踪性能。此外，我们还向您介绍了一些用于AR应用的高级渲染技术。最后，我们还展示了一些新颖的交互技术，您可以使用它们来创造出色的AR体验。这一章为您介绍了Android增强现实开发世界的入门。我们希望您已经准备好进入AR应用开发的新层次。
