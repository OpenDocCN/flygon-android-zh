- en: Chapter 5. Same as Hollywood – Virtual on Physical Objects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。与好莱坞相同——物理对象上的虚拟效果
- en: 'In the previous chapter you learned about the basic building blocks for implementing
    GPS and sensor-based AR applications. If you tried the different examples we presented,
    you might have noticed that the feeling of getting digital objects in real space
    (*registration*) works but can become coarse and unstable. This is mainly due
    to the accuracy problems of the used sensors (GPS, accelerometer, and so on) found
    in smartphones or tablet devices, and the characteristics of these technologies
    (for example, gyroscope drifting, GPS reliance on satellite visibility, and other
    such technologies). In this chapter, we will introduce you to a more robust solution,
    with it being the second major approach for supporting mobile AR: **Computer vision-based
    AR**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你已经学习了实现基于GPS和传感器的AR应用程序的基本构建块。如果你尝试了我们提供的不同示例，你可能会注意到将数字对象放置在真实空间中的感觉（*注册*）是可行的，但可能会变得粗糙且不稳定。这主要是由于智能手机或平板电脑中使用的传感器（如GPS、加速度计等）的准确性问题，以及这些技术的特性（例如，陀螺仪漂移、GPS对卫星可见性的依赖等）。在本章中，我们将介绍一种更健壮的解决方案，这是支持移动AR的第二种主要方法：**基于计算机视觉的AR**。
- en: Computer vision-based AR doesn't rely on any external sensors but uses the content
    of the camera image to support tracking, which is analysis through a flow of different
    algorithms. With computer vision-based AR, you get a better registration between
    the digital and physical worlds albeit at a little higher cost in terms of processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计算机视觉的AR不依赖于任何外部传感器，而是使用摄像头图像的内容来支持跟踪，这是通过不同算法流程的分析。使用基于计算机视觉的AR，你可以在数字和物理世界之间获得更好的注册效果，尽管在处理上的成本会稍微高一些。
- en: Probably, without even knowing it, you have already seen computer vision-based
    registration. If you go to see a blockbuster action movie with lots of cinematic
    effects, you will sometimes notice that some digital content has been overlaid
    over the physical recording set (for example, fake explosions, fake background,
    and fake characters running). In the same way as AR, the movie industry has to
    deal with the registration between digital and physical content, relying on analyzing
    the recorded image to recover tracking and camera information (using, for example,
    the match matchmoving technique). However, compared to Augmented Reality, it's
    done offline, and not in real time, generally relying on heavy workstations for
    registration and visual integration.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你甚至已经不知不觉中见过基于计算机视觉的注册效果。如果你去看一个充满电影特效的大片动作电影，有时你会注意到一些数字内容已经覆盖在物理录制场景上（例如，假的爆炸、假的背景和假的奔跑角色）。与AR一样，电影行业也必须处理数字和物理内容之间的注册问题，依靠分析录制的图像来恢复跟踪和相机信息（例如，使用匹配移动技术）。然而，与增强现实相比，它是离线完成的，不是实时完成，通常依赖于重型工作站进行注册和视觉整合。
- en: In this chapter, we will introduce you to the different types of computer vision-based
    tracking for AR. We will also describe to you the integration of a well-used and
    high-quality tracking library for mobile AR, **Vuforia^(TM)** by Qualcomm® Inc.
    With this library, we will be able to implement our first computer vision-based
    AR application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向你介绍不同类型的基于计算机视觉的AR跟踪。我们还将为你描述一个广泛使用且高质量的移动AR跟踪库——高通公司®的**Vuforia^(TM)**的集成。使用这个库，我们将能够实现我们的第一个基于计算机视觉的AR应用程序。
- en: Introduction to computer vision-based tracking and Vuforia^(TM)
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍基于计算机视觉的跟踪和Vuforia^(TM)
- en: So far, you have used the camera of the mobile phone exclusively for rendering
    the view of the real world as the background for your models. Computer vision-based
    AR goes a step further and processes each image frame to look for familiar *patterns*
    (or image features) in the camera image.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，你一直将手机的摄像头专门用于渲染真实世界的视图，作为你模型的背景。基于计算机视觉的AR更进了一步，它处理每一帧图像，寻找摄像头图像中熟悉的*模式*（或图像特征）。
- en: In a typical computer vision-based AR application, planar objects such as *frame
    markers* or *natural feature tracking targets* are used to position the camera
    in a *local coordinate system* (see [Chapter 3](ch03.html "Chapter 3. Superimposing
    the World"), *Superimposing the World*, *Figure showing the three most common
    coordinate systems*). This is in contrast to the global coordinate system (the
    earth) used in sensor-based AR but allows for more precise and stable overlay
    of virtual content in this local coordinate frame. Similar to before, obtaining
    the tracking information allows the updating of information about the virtual
    camera in our 3D graphics rendering engine and automatically provides us with
    registration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的基于计算机视觉的AR应用中，平面对象如*帧标记*或*自然特征追踪目标*被用来在*局部坐标系*中定位摄像头（请参阅[第3章](ch03.html
    "第3章. 覆盖世界")，*覆盖世界*，*显示三个最常见的坐标系统的图*）。这与基于传感器的AR中使用的全球坐标系（地球）相对立，但允许在此局部坐标框架内更精确和稳定地覆盖虚拟内容。与之前类似，获取追踪信息允许我们更新3D图形渲染引擎中虚拟摄像头的相关信息，并自动为我们提供注册。
- en: Choosing physical objects
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择物理对象
- en: 'In order to successfully implement computer vision-based AR, you need to understand
    which physical objects you can use to track the camera. Currently there are two
    major approaches to do this: Frame markers (**Fiducials**) and natural feature
    tracking targets (planar textured objects), as shown in the following figure.
    We will discuss both of them in the following section.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功实现基于计算机视觉的增强现实（AR），你需要了解哪些物理对象可以用来追踪摄像头。目前主要有两种方法可以实现这一点：帧标记（**Fiducials**）和平面纹理对象作为自然特征追踪目标，如下所示。在下一节中，我们将讨论这两种方法。
- en: '![Choosing physical objects](img/8553_05_01_FINAL.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![选择物理对象](img/8553_05_01_FINAL.jpg)'
- en: Understanding frame markers
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解帧标记
- en: In the early days of mobile Augmented Reality, it was of paramount importance
    to use computationally efficient algorithms. Computer vision algorithms are traditionally
    demanding as they generally rely on image analysis, complex geometric algorithms,
    and mathematical transformation, summing to a large number of operations that
    should take place at every time frame (to keep a constant frame rate at 30 Hz,
    you only have 33 ms for all that). Therefore, one of the first approaches to computer
    vision-based AR was to use relatively simple types of objects, which could be
    detected with computationally low-demanding algorithms, such as Fiducial markers.
    These markers are generally only defined at a grayscale level, simplifying their
    analysis and recognition in a traditional physical world (think about QR code
    but in 3D).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动增强现实技术的早期，使用计算效率高的算法至关重要。传统上，计算机视觉算法要求较高，因为它们通常依赖于图像分析、复杂的几何算法和数学变换，所有这些操作需要在每一帧内完成（为了保持30赫兹的恒定帧率，你只有33毫秒的时间）。因此，基于计算机视觉的AR的最初方法之一是使用相对简单的对象类型，这些对象可以用计算要求较低的算法检测，例如Fiducial标记。这些标记通常只在灰度级别定义，简化了在传统物理世界中的分析和识别（类似于3D中的二维码）。
- en: 'A typical algorithmic workflow for detecting these kinds of markers is depicted
    in the following figure and will be briefly explained next:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个典型的检测这类标记的算法流程，接下来将对其进行简要说明：
- en: '![Understanding frame markers](img/8553_05_02.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![理解帧标记](img/8553_05_02.jpg)'
- en: After an acquired camera image being converted to a grayscale image, the **threshold**
    is applied, that is, the grayscale level gets converted to a purely black and
    white image. The next step, **rectangle detection**, searches for edges in this
    simplified image, which is then followed by a process of detecting closed-contour,
    and potentially parallelogram shapes. Further steps are taken to ensure that the
    detected contour is really a parallelogram (that is, it has exactly four points
    and a couple of parallel lines). Once the shape is confirmed, the content of the
    marker is analyzed. A (binary) pattern within the border of the marker is extracted
    in the **pattern checking** step to *identify* the marker. This is important to
    be able to overlay different virtual content on different markers. For frame markers
    a simple bit code is used that supports 512 different combinations (and hence
    markers).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 获取的摄像机图像转换为灰度图像后，将应用**阈值**，即灰度级别被转换为纯黑白图像。下一步是**矩形检测**，在简化后的图像中搜索边缘，然后通过检测封闭轮廓的过程，可能是平行四边形形状。进一步的步骤是为了确保检测到的轮廓确实是一个平行四边形（即它恰好有四个点以及几条平行线）。一旦确认形状，就会分析标记的内容。在**模式检查**步骤中提取标记边框内的（二进制）模式以*识别*标记。这对于能够在不同的标记上叠加不同的虚拟内容非常重要。对于帧标记，使用一个简单的位编码，支持512种不同的组合（因此也支持512个不同的标记）。
- en: In the last step, the pose (that is the translation and rotation of the camera
    in the local coordinate system of the marker or reversely) is computed in the
    **pose estimation** step.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，通过**姿态估计**步骤计算姿态（即摄像机在标记局部坐标系统中的平移和旋转，反之亦然）。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Pose computation, in its simplest form a *homography* (a mapping between points
    on two planes), can be used together with the intrinsic parameters to recover
    the translation and rotation of the camera.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态计算，在其最简单的形式中是*homography*（两个平面上点之间的映射），可以与内在参数一起使用来恢复摄像机的平移和旋转。
- en: In practice, this is not a one-time computation, but rather, an iterative process
    in which the initial pose gets refined several times to obtain more accurate results.
    In order to reliably estimate the camera pose, the length of at least one side
    (the width or height) of the marker has to be known to the system; this is typically
    done through a configuration step when a marker description is loaded. Otherwise,
    the system could not tell reliably whether a small marker is near or a large marker
    is far away (due to the effects of perspective projection).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，这不是一次性的计算，而是一个迭代过程，初始姿态会经过多次细化以获得更准确的结果。为了可靠地估计摄像机姿态，至少需要让系统知道标记的一边（宽度或高度）的长度；这通常在加载标记描述时的配置步骤中完成。否则，系统可能无法可靠地判断一个小的标记是近还是大的标记是远（由于透视投影的影响）。
- en: Understanding natural feature tracking targets
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自然特征跟踪目标
- en: 'While the frame markers can be used to efficiently track the camera pose for
    many applications, you will want less obtrusive objects to track. You can achieve
    this by employing more advanced, but also computationally expensive, algorithms.
    The general idea of natural feature tracking is to use a number (in theory only
    three, and in practice several dozens or hundreds) of local points on a target
    to compute the camera pose. The challenge is that these points have to be reliable,
    robustly detected, and tracked. This is achieved with advanced computer vision
    algorithms to detect and describe the local neighborhood of an **interest point**
    (or feature point). Interest points have sharp, crisp details (such as corners),
    for example, using gradient orientations, which are suitable for feature points
    indicated by yellow crosses in the following figure. A circle or a straight line
    does not have sharp features and is not suitable for interest points:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管帧标记可以有效地用于许多应用中跟踪摄像机姿态，但你可能希望用不那么显眼的物体进行跟踪。通过使用更高级（但也计算成本更高）的算法，你可以实现这一点。自然特征跟踪的一般思想是使用目标上的多个（理论上只需三个，实际上则需要数十个或数百个）局部点来计算摄像机姿态。挑战在于这些点必须是可靠的，能够健壮地检测并跟踪。这是通过先进的计算机视觉算法来检测和描述**兴趣点**（或特征点）的局部邻域来实现的。兴趣点具有清晰的细节（如角落），例如，使用梯度方向，这适用于由黄色十字标记的特征点。圆形或直线没有清晰的细节，不适合作为兴趣点：
- en: '![Understanding natural feature tracking targets](img/8553_05_03.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![理解自然特征跟踪目标](img/8553_05_03.jpg)'
- en: 'Many feature points can be found on well-textured images (such as the image
    of the street used throughout this chapter):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在纹理丰富的图像上可以找到许多特征点（比如本章中使用的街道图像）：
- en: '![Understanding natural feature tracking targets](img/8553_05_04.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![理解自然特征跟踪目标](img/8553_05_04.jpg)'
- en: Beware that feature points cannot be well identified on images with homogenous
    color regions or soft edges (such as a blue sky or some computer graphics-rendered
    pictures).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在颜色均匀区域或边缘柔和的图像上（如蓝天或一些计算机图形渲染的图片），特征点无法被很好地识别。
- en: Vuforia^(TM) architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vuforia^(TM)架构
- en: 'Vuforia^(TM) is an Augmented Reality library distributed by Qualcomm® Inc.
    The library is free for use in non-commercial or commercial projects. The library
    supports frame marker and natural feature target tracking as well as multi-target,
    which are combinations of multiple targets. The library also features basic rendering
    functions (video background and OpenGL® 3D rendering), linear algebra (matrix/vector
    transformation), and interaction capabilities (virtual buttons). The library is
    actually available on both iOS and Android platforms, and the performance is improved
    on mobile devices equipped with Qualcomm® chipsets. An overview of the library
    architecture is presented in the following figure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Vuforia^(TM)是由高通公司®分发的增强现实库。该库在非商业或商业项目中免费使用。库支持帧标记和自然特征目标跟踪以及多目标，这是多个目标的组合。库还具备基本的渲染功能（视频背景和OpenGL®
    3D渲染）、线性代数（矩阵/向量变换）以及交互能力（虚拟按钮）。实际上，该库在iOS和Android平台上都可以使用，并且在配备高通®芯片组的移动设备上性能有所提升。以下图展示了库架构的概览：
- en: '![VuforiaTM architecture](img/8553_05_05.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![VuforiaTM架构](img/8553_05_05.jpg)'
- en: The architecture, from a client viewpoint (application box on the left of the
    preceding figure), offers a state object to the developer, which contains information
    about recognized targets as well as the camera content. We won't get into too
    much of details here as a list of samples is available on their website, along
    with full documentation and an active forum, at [http://developer.vuforia.com/](http://developer.vuforia.com/).
    What you need to know is that the library uses the **Android NDK** for its integration
    as it's being developed in C++.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端视角来看（如前图左边的应用程序框），架构为开发者提供了一个状态对象，其中包含有关已识别目标以及相机内容的信息。这里我们不会详细介绍，因为他们的网站上有一系列示例，以及完整的文档和一个活跃的论坛，请访问[http://developer.vuforia.com/](http://developer.vuforia.com/)。你需要知道的是，该库使用**Android
    NDK**进行集成，因为它是用C++开发的。
- en: This is mainly due to the gains of high-performance computation for image analysis
    or computer vision with C++ rather than doing it in Java (concurrent technologies
    also use the same approach). It's a drawback for us (as we are using JME and Java
    only) but a gain for you in terms of getting performances in your application.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这主要是因为使用C++进行图像分析或计算机视觉的高性能计算收益，而不是用Java（并发技术也采用相同的方法）。这对于我们来说是一个缺点（因为我们只使用JME和Java），但对于你在应用程序中获得性能来说是一个收益。
- en: 'To use the library, you generally need to follow these three steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个库，你通常需要遵循以下三个步骤：
- en: Train and create your target or markers
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练并创建你的目标或标记
- en: Integrate the library in your application
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的应用程序中集成库
- en: Deploy your application
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署你的应用程序
- en: In the next section, we will introduce you to creating and training your targets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何创建和训练你的目标。
- en: Configuring Vuforia^(TM) to recognize objects
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Vuforia^(TM)以识别对象
- en: To use the Vuforia^(TM) toolkit with natural feature tracking targets, first
    you need to create them. In the recent version of the library (2.0), you can automatically
    create your target when the application is running (online) or predefine them
    before deploying your application (offline). We will show you how to proceed for
    offline creation. First go to the Vuforia^(TM) developer website [https://developer.vuforia.com](https://developer.vuforia.com).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用带有自然特征跟踪目标的Vuforia^(TM)工具包，首先你需要创建它们。在库的最新版本（2.0）中，你可以在应用程序运行时（在线）自动创建你的目标，或者在部署应用程序之前（离线）预先定义它们。我们将向你展示如何离线创建。首先访问Vuforia^(TM)开发者网站[https://developer.vuforia.com](https://developer.vuforia.com)。
- en: 'The first thing you need to do is to log in to the website to access the tool
    for creating your target. Click on the upper-right corner and register if you
    have not done it before. After login, you can click on **Target Manager**, the
    training program to create targets. The target manager is organized in a database
    (which can correspond to your project), and for database, you can create a list
    of targets, as shown in the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的第一件事是登录到网站，以访问创建你目标的工具。点击右上角，如果你之前没有做过，请注册。登录后，你可以点击**目标管理器**，这是创建目标的培训计划。目标管理器以数据库的形式组织（可以对应你的项目），对于数据库，你可以创建一个目标列表，如下截图所示：
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_07.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![配置VuforiaTM以识别物体](img/8553_05_07.jpg)'
- en: 'So let''s create our first database. Click on **Create Database**, and enter
    `VuforiaJME`. Your database should appear in your **Device Databases** list. Select
    it to get onto the following page:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第一个数据库。点击**创建数据库**，并输入`VuforiaJME`。你的数据库应该会出现在**设备数据库**列表中。选择它进入下一页：
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_08.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![配置VuforiaTM以识别物体](img/8553_05_08.jpg)'
- en: 'Click on **Add New Target** to create the first target. A dialog box will appear
    with different text fields to complete, as shown in the following screenshot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**添加新目标**以创建第一个目标。会出现一个对话框，包含不同文本字段以填写，如下截图所示：
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_09.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![配置VuforiaTM以识别物体](img/8553_05_09.jpg)'
- en: 'First you need to pick up a name for your target; in our case, we will call
    it `VuforiaJMETarget`. Vuforia^(TM) allows you to create different types of targets
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你需要为你的目标选择一个名字；在我们的例子中，我们将其称为`VuforiaJMETarget`。Vuforia^(TM)允许你创建以下不同类型的目标：
- en: '**Single Image**: You create only one planar surface and use only one image.
    The target is generally used for printing on a page, part of a magazine, and so
    on.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单张图片**：你只创建一个平面表面，并且只使用一张图片。目标通常用于在页面、杂志的一部分等上打印。'
- en: '**Cube**: You define multiple surfaces (with multiple pictures), which will
    be used to track a 3D cube. This can be used for games, packaging, and so on.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**立方体**：你定义多个表面（带有多张图片），将用于追踪一个3D立方体。这可以用于游戏、包装等。'
- en: '**Cuboid**: It''s a variation of the cube type, as a parallelepiped with non-square
    faces.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长方体**：这是立方体类型的变化，具有非正方形面的平行六面体。'
- en: 'Select **Single Image** target type. The target dimension defines a relative
    scale for your marker. The unit is not defined as it corresponds to the size of
    your virtual object. A good tip is to consider that everything is in centimeters
    or millimeters, which is generally the size of your physical marker (for example,
    print on an A4 or letter page). In our case, we enter the dimension in centimeters.
    Finally, you need to select an image which will be used for the target. As an
    example, you can select the `stones.jpg` image, which is available with the Vuforia^(TM)
    sample distribution (Media directory in the *ImageTargets* example on the Vuforia^(TM)
    website). To validate your configuration, click on **Add**, and wait as the image
    is being processed. When the processing is over, you should get a screen like
    the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**单张图片**目标类型。目标尺寸为你的标记定义了一个相对比例。单位没有定义，因为它对应于你的虚拟对象的大小。一个很好的建议是考虑所有尺寸都是以厘米或毫米为单位，这通常是你的物理标记的大小（例如，打印在A4或信纸上）。在我们的例子中，我们以厘米为单位输入尺寸。最后，你需要选择一个将用于目标的图像。例如，你可以选择`stones.jpg`图像，这是Vuforia^(TM)示例发行版中提供的（在Vuforia^(TM)网站上的*ImageTargets*示例的媒体目录中）。为验证你的配置，点击**添加**，然后等待图像处理。处理完成后，你应该会看到一个如下所示的屏幕：
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_10.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![配置VuforiaTM以识别物体](img/8553_05_10.jpg)'
- en: 'The stars notify you of the quality of the target for tracking. This example
    has five stars, which means it will work really well. You can get more information
    on the Vuforia^(TM) website on how to create a good image for a target: [https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating](https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 星级会告诉你追踪目标的品质如何。这个例子有五颗星，意味着它将非常好用。你可以在Vuforia^(TM)网站上获取更多信息，了解如何为追踪目标创建一个好的图像：[https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating](https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating)。
- en: Our last step is now to export the created target. So select the target (tick
    the box next to **VuforiaJMETarget**), and click on **Download Selected Targets**.
    On the dialog box that appears, choose **SDK** for export and **VuforiaJME** for
    our database name, and save.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的最后一步是导出已创建的目标。因此，选择目标（勾选**VuforiaJMETarget**旁边的框），然后点击**下载选择的目标**。在出现的对话框中，选择**SDK**作为导出，**VuforiaJME**作为我们的数据库名称，然后保存。
- en: '![Configuring VuforiaTM to recognize objects](img/8853_05_11.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![配置VuforiaTM以识别对象](img/8853_05_11.jpg)'
- en: 'Unzip your compressed file. You will see two files: a `.dat` file and a `.xml`
    file. Both files are used for operating the Vuforia^(TM) tracking at runtime.
    The `.dat` file specifies the feature points from your image and the `.xml` file
    is a configuration file. Sometimes you may want to change the size of your marker
    or do some basic editing without having to restart or do the training; you can
    modify it directly on your XML file. So now we are ready with our target for implementing
    our first Vuforia^(TM) project!'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解压你压缩的文件。你会看到两个文件：一个`.dat`文件和一个`.xml`文件。这两个文件都用于在运行时操作Vuforia^(TM)追踪。`.dat`文件指定了你的图像中的特征点，而`.xml`文件是一个配置文件。有时你可能想要更改标记的大小或进行一些基本编辑，而不必重新启动或进行训练；你可以直接在XML文件上进行修改。现在我们已经准备好实现我们的第一个Vuforia^(TM)项目的目标了！
- en: Putting it together – Vuforia^(TM) with JME
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将它们组合在一起——Vuforia^(TM)与JME
- en: 'In this section we will show you how to integrate Vuforia^(TM) with JME. We
    will use a natural feature-tracking target for this purpose. So open the **VuforiaJME**
    project in your Eclipse to start. As you can already observe, there are two main
    changes compared to our previous projects:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向你展示如何将Vuforia^(TM)与JME集成。我们将使用自然特征追踪目标来实现这一目的。因此，在Eclipse中打开**VuforiaJME**项目以开始。正如你已经可以观察到的，与我们的前一个项目相比，有两个主要变化：
- en: The camera preview class is gone
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相机预览类已移除
- en: There is a new directory in the project root named `jni`
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目根目录中有一个名为`jni`的新目录。
- en: The first change is due to the way Vuforia^(TM) manages the camera. Vuforia^(TM)
    uses its own camera handle and camera preview integrated in the library. Therefore,
    we'll need to query the video image through the Vuforia^(TM) library to display
    it on our scene graph (using the same principle as seen in [Chapter 2](ch02.html
    "Chapter 2. Viewing the World"), *Viewing the World*).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次更改是由于Vuforia^(TM)管理相机的方式。Vuforia^(TM)使用自己的相机句柄和集成在库中的相机预览。因此，我们需要通过Vuforia^(TM)库查询视频图像，以便在我们的场景图中显示（使用与[第2章](ch02.html
    "第2章。观看世界")相同的原理，*观看世界*）。
- en: The `jni` folder contains C++ source code, which is required for Vuforia^(TM).
    To integrate Vuforia^(TM) with JME, we need to interoperate Vuforia's low-level
    part (C++) with the high-level part (Java). It means we will need to compile C++
    and Java code and transfer data between them. If you have done it, you'll need
    to download and install the Android NDK before going further (as explained in
    [Chapter 1](ch01.html "Chapter 1. Augmented Reality Concepts and Tools"), *Augmented
    Reality Concepts and Tools*).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`jni`文件夹包含C++源代码，这是Vuforia^(TM)所需的。为了将Vuforia^(TM)与JME集成，我们需要互操作Vuforia^(TM)的低级别部分（C++）和高级别部分（Java）。这意味着我们将需要编译C++和Java代码并在它们之间传输数据。如果你做到了，你将需要在继续之前下载并安装Android
    NDK（如[第1章](ch01.html "第1章。增强现实概念和工具")所述，*增强现实概念和工具*）。'
- en: The C++ integration
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C++集成
- en: 'The C++ layer is based on a modified version of the **ImageTargets** example
    available on the Vuforia^(TM) website. The `jni` folder contains the following
    files:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: C++层基于Vuforia^(TM)网站上提供的**ImageTargets**示例的修改版本。`jni`文件夹包含以下文件：
- en: '`MathUtils.cpp` and `MathUtils.h`: Utilities functions for mathematical computation'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MathUtils.cpp`和`MathUtils.h`：用于数学计算的实用功能函数'
- en: '`VuforiaNative.cpp`: This is the main C++ class that interacts with our Java
    layer'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VuforiaNative.cpp`：这是与我们的Java层交互的主要C++类'
- en: '`Android.mk` and `Application.mk`: These contains configuration files for compilation'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Android.mk`和`Application.mk`：这些包含编译配置文件'
- en: Open the `Android.mk` file, and check if the path to your Vuforia^(TM) installation
    is correct in the `QCAR_DIR` directory. Use only a relative path to make it cross-platform
    (on MacOS with the android ndk r9 or higher, an absolute path will be concatenated
    with the current directory and result in an incorrect directory path).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Android.mk`文件，检查到你的Vuforia^(TM)安装路径在`QCAR_DIR`目录中是否正确。使用相对路径使其跨平台（在MacOS上使用android
    ndk r9或更高版本，绝对路径将与当前目录拼接，导致不正确的目录路径）。
- en: 'Now open the `VuforiNative.cpp` file. A lot of functions are defined in the
    files but only three are relevant to us:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开`VuforiNative.cpp`文件。文件中定义了很多函数，但只有三个与我们有关系：
- en: '`Java_com_ar4android_VuforiaJMEActivity_loadTrackerData(JNIEnv *, jobject)`:
    This is the function for loading our specific target (created in the previous
    section)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Java_com_ar4android_VuforiaJMEActivity_loadTrackerData(JNIEnv *, jobject)`:
    这是用于加载我们特定目标（在上一节中创建）的函数'
- en: '`virtual void QCAR_onUpdate(QCAR::State& state)`: This is the function to query
    the camera image and transfer it to the Java layer'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`virtual void QCAR_onUpdate(QCAR::State& state)`: 这是查询相机图像并将其传递给Java层的函数'
- en: '`Java_com_ar4android_VuforiaJME_updateTracking(JNIEnv *env, jobject obj)`:
    This function is used to query the position of the targets and transfer it to
    the Java layer'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Java_com_ar4android_VuforiaJME_updateTracking(JNIEnv *env, jobject obj)`:
    这个函数用于查询目标的位置并将其传递给Java层'
- en: 'The first step will be to use our specific target in our application and the
    first function. So copy and paste the `VuforiaJME.dat` and `VuforiaJME.xml` files
    to your assets directory (there should already be two target configurations).
    Vuforia^(TM) configures the target that will be used based on the XMLconfiguration
    file. `loadTrackerData` gets first access to `TrackerManager` and `imageTracker`
    (which is a tracker for non-natural features):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将是在我们的应用程序中使用特定目标以及第一个函数。因此，将`VuforiaJME.dat`和`VuforiaJME.xml`文件复制并粘贴到你的资产目录中（应该已经有两个目标配置）。Vuforia^(TM)根据XML配置文件配置将使用的目标。`loadTrackerData`首先访问`TrackerManager`和`imageTracker`（用于非自然特征的追踪器）：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next step is to create a specific target, such as instancing a dataset.
    In this example, one dataset is created, named `dataSetStonesAndChips`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个特定的目标，比如实例化一个数据集。在这个例子中，创建了一个名为`dataSetStonesAndChips`的数据集：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After we load the configuration of the targets in the created instance, this
    is where we set up our VuforiaJME target:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建的实例中加载目标的配置后，这里是我们设置VuforiaJME目标的地方：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally we can activate the dataset by calling the `activateDataSet` function.
    If you don''t activate the dataset, the target will be loaded and initialized
    in the tracker but won''t be tracked until activation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用`activateDataSet`函数来激活数据集。如果你不激活数据集，目标将在追踪器中加载和初始化，但在激活之前不会被追踪：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have our target initialized, we need to get the real view of the world
    with Vuforia^(TM). The concept is the same as we have seen before: using a video
    background camera in the JME class and updating it with an image. However, here,
    the image is not coming from a Java `Camera.PreviewCallback` but from Vuforia^(TM).
    In Vuforia^(TM) the best place to get the video image is in the `QCAR_onUpdate`
    function. This function is called just after the tracker gets updated. An image
    can be retrieved by querying a frame on the State object of Vuforia^(TM) with
    `getFrame()`. A frame can contain multiple images, as the camera image is in different
    formats (for example, YUV, RGB888, GREYSCALE, RGB565, and so on). In the previous
    example, we used the RGB565 format in our JME class. We will do the same here.
    So our class will start as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们初始化了目标，就需要使用Vuforia^(TM)获取现实世界的真实视图。这个概念与我们之前看到的相同：在JME类中使用视频背景相机并使用图像更新它。然而，在这里，图像不是来自Java的`Camera.PreviewCallback`，而是来自Vuforia^(TM)。在Vuforia^(TM)中获取视频图像的最佳位置是在`QCAR_onUpdate`函数中。这个函数在追踪器更新后立即被调用。可以通过查询Vuforia^(TM)的状态对象的帧来获取图像，使用`getFrame()`。一个帧可能包含多个图像，因为相机图像有不同的格式（例如，YUV、RGB888、GREYSCALE、RGB565等）。在之前的例子中，我们在JME类中使用了RGB565格式。这里我们也将这样做。所以我们的类将从这里开始。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function parses a list of images in the frame and retrieves the `RGB565`
    image. Once we get this image, we need to transfer it to the **Java Layer**. For
    doing that you can use a JNI function:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数解析帧中的图像列表并获取`RGB565`图像。一旦我们得到这个图像，我们需要将其传递给**Java层**。为此，你可以使用JNI函数：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, we get information about the size of the image and a pointer
    on the raw data of the image. We use a JNI function named `setRGB565CameraImage`,
    which is defined in our `Java Activity` class. We call this function from C++
    and pass in argument the content of the image (`pixelArray`) as `width` and `height`
    of the image. So each time the tracker updates, we retrieve a new camera image
    and send it to the Java layer by calling the `setRGB565CameraImage` function.
    The JNI mechanism is really useful and you can use it for passing any data, from
    a sophisticated computation process back to your Java class (for example, physics,
    numerical simulation, and so on).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们获取关于图像大小以及图像原始数据的指针。我们使用名为`setRGB565CameraImage`的JNI函数，该函数在我们的`Java
    Activity`类中定义。我们从C++中调用这个函数，并传入图像内容（`pixelArray`）作为图像的`width`和`height`。因此，每次追踪器更新时，我们都会获取新的摄像头图像，并通过调用`setRGB565CameraImage`函数将其发送到Java层。JNI机制非常有用，你可以使用它来传递任何数据，从复杂的计算过程回到你的Java类（例如，物理，数值模拟等）。
- en: 'The next step is to retrieve the location of the targets from the tracking.
    We will do that from the `updateTracking` function. As before, we get an instance
    of the State object from Vuforia^(TM). The State object contains `TrackableResults`,
    which is a list of the identified targets in the video image (identified here
    as being recognized as a target and their position identified):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从追踪中获取目标的位置。我们将在`updateTracking`函数中执行此操作。像之前一样，我们从Vuforia^(TM)获取State对象的实例。State对象包含`TrackableResults`，这是视频图像中识别的目标列表（在这里被识别为目标及其位置）：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In our example, we have only one target activated, so if we get a result, it
    will obviously be our marker. We can then directly query the position information
    from it. If you had multiple activated markers, you will need to identify which
    one is which, by getting information from the result by calling `result->getTrackable()`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，只有一个目标被激活，所以如果我们得到一个结果，它显然将是我们的标记。然后我们可以直接查询它的位置信息。如果你有多个激活的标记，你将需要通过调用`result->getTrackable()`从结果中获取信息，以确定哪个是哪个。
- en: 'The position of `trackable` is queried by calling `result->getPose()`, which
    returns a matrix defining a linear transformation. This transformation gives you
    the position of the marker relative to the camera position. Vuforia^(TM) uses
    a computer-vision coordinate system (x on the left, y down, and z away from you),
    which is different from JME, so we will have to do some conversion later on. For
    now, what we will do first is inverse the transformation, to get the position
    of the camera relative to the marker; this will make the marker the reference
    coordinate system for our virtual content. So you will do some basic mathematical
    operations as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`result->getPose()`来查询`trackable`的位置，这将返回一个定义线性变换的矩阵。这个变换可以给出标记相对于摄像头位置的位置。Vuforia^(TM)使用的是计算机视觉坐标系（x向左，y向下，z远离你），这与JME不同，因此我们稍后需要进行一些转换。现在，我们首先要做的是反转变换，以得到相对于标记的摄像头位置；这将使标记成为我们虚拟内容的参考坐标系。所以你将进行以下一些基本的数学运算：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we have the location (`cam_x,y,z`) as well as the orientation of our camera
    `(cam_right_/cam_up_/cam_dir_x,y,z`).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了摄像头的位置（`cam_x,y,z`）以及摄像头的方向（`cam_right_/cam_up_/cam_dir_x,y,z`）。
- en: 'The last step is to transfer this information to the Java layer. We will use
    JNI again for this operation. What we also need is information about the internal
    parameters of our camera. This is similar to what was discussed in [Chapter 3](ch03.html
    "Chapter 3. Superimposing the World"), *Superimposing the World*, but now it has
    been done here with Vuforia^(TM). For that, you can access the `CameraCalibration`
    object from `CameraDevice`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将这些信息传递到Java层。我们将再次使用JNI进行此操作。我们还需要的是关于我们摄像头内部参数的信息。这与[第3章](ch03.html "第3章.叠加世界")中讨论的内容相似，*叠加世界*，但现在这里使用Vuforia^(TM)完成。为此，你可以从`CameraDevice`访问`CameraCalibration`对象：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can easily transform the projection transformation to a more readable format
    for the camera configuration, such as its field of view (`fovDegrees`), which
    we also have to adapt to allow for differences in the aspect ratios of the camera
    sensor and the screen:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地将投影变换转换为更易于阅读的摄像头配置格式，比如其视场（`fovDegrees`），我们也必须调整它以适应摄像头传感器和屏幕的宽高比差异：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then call three JNI functions to transfer the field of view (`setCameraPerspectiveNative`),
    camera position (`setCameraPoseNative`) and camera orientation (`setCameraOrientationNative`)
    to our Java layer. These three functions are time defined in the `VuforiaJME`
    class, which allows us to quickly modify our virtual camera:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用三个 JNI 函数，将视场（`setCameraPerspectiveNative`）、摄像头位置（`setCameraPoseNative`）和摄像头方向（`setCameraOrientationNative`）传输到我们的
    Java 层。这三个函数在 `VuforiaJME` 类中有定义，这使得我们可以快速修改我们的虚拟摄像头：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The last step will be to compile the program. So run a command shell, and go
    the `jni` directory containing the files. From there you need to call the `ndk-build`
    function. The function is defined in your `android-ndk-r9d` directory, so be sure
    it''s accessible from your path. If everything goes well, you should see the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步将是编译程序。所以，运行一个命令行窗口，前往包含文件的 `jni` 目录。从那里你需要调用 `ndk-build` 函数。该函数在你的 `android-ndk-r9d`
    目录中定义，所以确保它可以从你的路径中访问。如果一切顺利，你应该会看到以下内容：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Time to go back to Java!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候回到 Java 了！
- en: The Java integration
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java 集成
- en: The Java layer defines the function previously called using similar classes
    from our *Superimpose* example. The first function is the `setRGB565CameraImage`
    function which handles the video image as in the previous examples.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Java 层定义了之前使用与我们的 *Superimpose* 示例相似的类调用的函数。第一个函数是 `setRGB565CameraImage`，它处理视频图像，如之前的例子所示。
- en: The other JNI functions will modify the characteristics of our foreground camera.
    Specifically, we modify the left axis of the JME camera to match the coordinate
    system used by Vuforia^(TM) (as depicted in the figure in the *Choosing physical
    objects* section).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 JNI 函数将修改我们前台摄像头的特性。具体来说，我们会调整 JME 摄像头的左侧轴，以匹配 Vuforia^(TM) 使用的坐标系（如图中*选择物理对象*一节所示）。
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we have to adjust the viewport of the background camera, which shows
    the camera image, to prevent 3D objects from floating above the physical target:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须调整显示摄像头图像的背景摄像头的视口，以防止 3D 对象漂浮在物理目标之上：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And that''s it. What we want to outline again here is the concept behind it:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么多。我们再次想要强调的是背后的概念：
- en: The camera model used in your tracker is matched with your virtual camera (in
    this example `CameraCalibration` from Vuforia^(TM) to our JME Virtual Camera).
    This will guarantee us a correct registration.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的跟踪器中使用的摄像头模型与你的虚拟摄像头（在这个例子中，Vuforia^(TM) 的 `CameraCalibration` 与我们的 JME 虚拟摄像头）相匹配。这将保证我们正确的注册。
- en: You track a target in your camera coordinate system (in this example, a natural
    feature target from Vuforia^(TM)). This tracking replaces our GPS as seen previously,
    and uses a local coordinate system.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在摄像头坐标系中跟踪一个目标（在这个例子中，是来自 Vuforia^(TM) 的自然特征目标）。这种跟踪取代了我们之前看到的 GPS，并使用了一个局部坐标系。
- en: The position of this target is used to modify the pose of your virtual camera
    (in this example, transferring the detected position from C++ to Java with JNI,
    and updating our JME Virtual Camera). As we repeat the process for each frame,
    we have a full 6DOF registration between physical (the target) and virtual (our
    JME scene).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个目标的位置被用来修改你的虚拟摄像头的姿态（在这个例子中，通过 JNI 将检测到的位置从 C++ 传输到 Java，并更新我们的 JME 虚拟摄像头）。由于我们对每一帧都重复这个过程，因此在物理（目标）和虚拟（我们的
    JME 场景）之间有一个完整的 6DOF 注册。
- en: 'Your results should look similar to the one in the following figure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果应该与下面这幅图类似：
- en: '![The Java integration](img/8553_05_12.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Java 集成](img/8553_05_12.jpg)'
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced you to computer vision-based AR. We developed
    an application with the Vuforia^(TM) library and showed you how to integrate it
    with JME. You are now ready to create natural feature tracking-based AR applications.
    In this demo, you can move your device around the marker and see the virtual content
    from every direction. In the next chapter, we will learn how we can do more in
    terms of interaction. How about being able to select the model and play with it?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了基于计算机视觉的 AR。我们使用 Vuforia^(TM) 库开发了一个应用程序，并展示了如何将其与 JME 集成。你现在可以创建基于自然特征跟踪的
    AR 应用程序了。在这个演示中，你可以围绕标记移动你的设备，并从各个方向看到虚拟内容。在下一章中，我们将学习如何进行更多交互。比如能够选择模型并与之互动怎么样？
