- en: Chapter 5. Same as Hollywood – Virtual on Physical Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter you learned about the basic building blocks for implementing
    GPS and sensor-based AR applications. If you tried the different examples we presented,
    you might have noticed that the feeling of getting digital objects in real space
    (*registration*) works but can become coarse and unstable. This is mainly due
    to the accuracy problems of the used sensors (GPS, accelerometer, and so on) found
    in smartphones or tablet devices, and the characteristics of these technologies
    (for example, gyroscope drifting, GPS reliance on satellite visibility, and other
    such technologies). In this chapter, we will introduce you to a more robust solution,
    with it being the second major approach for supporting mobile AR: **Computer vision-based
    AR**.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision-based AR doesn't rely on any external sensors but uses the content
    of the camera image to support tracking, which is analysis through a flow of different
    algorithms. With computer vision-based AR, you get a better registration between
    the digital and physical worlds albeit at a little higher cost in terms of processing.
  prefs: []
  type: TYPE_NORMAL
- en: Probably, without even knowing it, you have already seen computer vision-based
    registration. If you go to see a blockbuster action movie with lots of cinematic
    effects, you will sometimes notice that some digital content has been overlaid
    over the physical recording set (for example, fake explosions, fake background,
    and fake characters running). In the same way as AR, the movie industry has to
    deal with the registration between digital and physical content, relying on analyzing
    the recorded image to recover tracking and camera information (using, for example,
    the match matchmoving technique). However, compared to Augmented Reality, it's
    done offline, and not in real time, generally relying on heavy workstations for
    registration and visual integration.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to the different types of computer vision-based
    tracking for AR. We will also describe to you the integration of a well-used and
    high-quality tracking library for mobile AR, **Vuforia^(TM)** by Qualcomm® Inc.
    With this library, we will be able to implement our first computer vision-based
    AR application.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to computer vision-based tracking and Vuforia^(TM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, you have used the camera of the mobile phone exclusively for rendering
    the view of the real world as the background for your models. Computer vision-based
    AR goes a step further and processes each image frame to look for familiar *patterns*
    (or image features) in the camera image.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical computer vision-based AR application, planar objects such as *frame
    markers* or *natural feature tracking targets* are used to position the camera
    in a *local coordinate system* (see [Chapter 3](ch03.html "Chapter 3. Superimposing
    the World"), *Superimposing the World*, *Figure showing the three most common
    coordinate systems*). This is in contrast to the global coordinate system (the
    earth) used in sensor-based AR but allows for more precise and stable overlay
    of virtual content in this local coordinate frame. Similar to before, obtaining
    the tracking information allows the updating of information about the virtual
    camera in our 3D graphics rendering engine and automatically provides us with
    registration.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing physical objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to successfully implement computer vision-based AR, you need to understand
    which physical objects you can use to track the camera. Currently there are two
    major approaches to do this: Frame markers (**Fiducials**) and natural feature
    tracking targets (planar textured objects), as shown in the following figure.
    We will discuss both of them in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing physical objects](img/8553_05_01_FINAL.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding frame markers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the early days of mobile Augmented Reality, it was of paramount importance
    to use computationally efficient algorithms. Computer vision algorithms are traditionally
    demanding as they generally rely on image analysis, complex geometric algorithms,
    and mathematical transformation, summing to a large number of operations that
    should take place at every time frame (to keep a constant frame rate at 30 Hz,
    you only have 33 ms for all that). Therefore, one of the first approaches to computer
    vision-based AR was to use relatively simple types of objects, which could be
    detected with computationally low-demanding algorithms, such as Fiducial markers.
    These markers are generally only defined at a grayscale level, simplifying their
    analysis and recognition in a traditional physical world (think about QR code
    but in 3D).
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical algorithmic workflow for detecting these kinds of markers is depicted
    in the following figure and will be briefly explained next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding frame markers](img/8553_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After an acquired camera image being converted to a grayscale image, the **threshold**
    is applied, that is, the grayscale level gets converted to a purely black and
    white image. The next step, **rectangle detection**, searches for edges in this
    simplified image, which is then followed by a process of detecting closed-contour,
    and potentially parallelogram shapes. Further steps are taken to ensure that the
    detected contour is really a parallelogram (that is, it has exactly four points
    and a couple of parallel lines). Once the shape is confirmed, the content of the
    marker is analyzed. A (binary) pattern within the border of the marker is extracted
    in the **pattern checking** step to *identify* the marker. This is important to
    be able to overlay different virtual content on different markers. For frame markers
    a simple bit code is used that supports 512 different combinations (and hence
    markers).
  prefs: []
  type: TYPE_NORMAL
- en: In the last step, the pose (that is the translation and rotation of the camera
    in the local coordinate system of the marker or reversely) is computed in the
    **pose estimation** step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pose computation, in its simplest form a *homography* (a mapping between points
    on two planes), can be used together with the intrinsic parameters to recover
    the translation and rotation of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this is not a one-time computation, but rather, an iterative process
    in which the initial pose gets refined several times to obtain more accurate results.
    In order to reliably estimate the camera pose, the length of at least one side
    (the width or height) of the marker has to be known to the system; this is typically
    done through a configuration step when a marker description is loaded. Otherwise,
    the system could not tell reliably whether a small marker is near or a large marker
    is far away (due to the effects of perspective projection).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding natural feature tracking targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the frame markers can be used to efficiently track the camera pose for
    many applications, you will want less obtrusive objects to track. You can achieve
    this by employing more advanced, but also computationally expensive, algorithms.
    The general idea of natural feature tracking is to use a number (in theory only
    three, and in practice several dozens or hundreds) of local points on a target
    to compute the camera pose. The challenge is that these points have to be reliable,
    robustly detected, and tracked. This is achieved with advanced computer vision
    algorithms to detect and describe the local neighborhood of an **interest point**
    (or feature point). Interest points have sharp, crisp details (such as corners),
    for example, using gradient orientations, which are suitable for feature points
    indicated by yellow crosses in the following figure. A circle or a straight line
    does not have sharp features and is not suitable for interest points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding natural feature tracking targets](img/8553_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Many feature points can be found on well-textured images (such as the image
    of the street used throughout this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding natural feature tracking targets](img/8553_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Beware that feature points cannot be well identified on images with homogenous
    color regions or soft edges (such as a blue sky or some computer graphics-rendered
    pictures).
  prefs: []
  type: TYPE_NORMAL
- en: Vuforia^(TM) architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vuforia^(TM) is an Augmented Reality library distributed by Qualcomm® Inc.
    The library is free for use in non-commercial or commercial projects. The library
    supports frame marker and natural feature target tracking as well as multi-target,
    which are combinations of multiple targets. The library also features basic rendering
    functions (video background and OpenGL® 3D rendering), linear algebra (matrix/vector
    transformation), and interaction capabilities (virtual buttons). The library is
    actually available on both iOS and Android platforms, and the performance is improved
    on mobile devices equipped with Qualcomm® chipsets. An overview of the library
    architecture is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![VuforiaTM architecture](img/8553_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The architecture, from a client viewpoint (application box on the left of the
    preceding figure), offers a state object to the developer, which contains information
    about recognized targets as well as the camera content. We won't get into too
    much of details here as a list of samples is available on their website, along
    with full documentation and an active forum, at [http://developer.vuforia.com/](http://developer.vuforia.com/).
    What you need to know is that the library uses the **Android NDK** for its integration
    as it's being developed in C++.
  prefs: []
  type: TYPE_NORMAL
- en: This is mainly due to the gains of high-performance computation for image analysis
    or computer vision with C++ rather than doing it in Java (concurrent technologies
    also use the same approach). It's a drawback for us (as we are using JME and Java
    only) but a gain for you in terms of getting performances in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the library, you generally need to follow these three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train and create your target or markers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate the library in your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will introduce you to creating and training your targets.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Vuforia^(TM) to recognize objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use the Vuforia^(TM) toolkit with natural feature tracking targets, first
    you need to create them. In the recent version of the library (2.0), you can automatically
    create your target when the application is running (online) or predefine them
    before deploying your application (offline). We will show you how to proceed for
    offline creation. First go to the Vuforia^(TM) developer website [https://developer.vuforia.com](https://developer.vuforia.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you need to do is to log in to the website to access the tool
    for creating your target. Click on the upper-right corner and register if you
    have not done it before. After login, you can click on **Target Manager**, the
    training program to create targets. The target manager is organized in a database
    (which can correspond to your project), and for database, you can create a list
    of targets, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So let''s create our first database. Click on **Create Database**, and enter
    `VuforiaJME`. Your database should appear in your **Device Databases** list. Select
    it to get onto the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on **Add New Target** to create the first target. A dialog box will appear
    with different text fields to complete, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'First you need to pick up a name for your target; in our case, we will call
    it `VuforiaJMETarget`. Vuforia^(TM) allows you to create different types of targets
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Image**: You create only one planar surface and use only one image.
    The target is generally used for printing on a page, part of a magazine, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cube**: You define multiple surfaces (with multiple pictures), which will
    be used to track a 3D cube. This can be used for games, packaging, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cuboid**: It''s a variation of the cube type, as a parallelepiped with non-square
    faces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Select **Single Image** target type. The target dimension defines a relative
    scale for your marker. The unit is not defined as it corresponds to the size of
    your virtual object. A good tip is to consider that everything is in centimeters
    or millimeters, which is generally the size of your physical marker (for example,
    print on an A4 or letter page). In our case, we enter the dimension in centimeters.
    Finally, you need to select an image which will be used for the target. As an
    example, you can select the `stones.jpg` image, which is available with the Vuforia^(TM)
    sample distribution (Media directory in the *ImageTargets* example on the Vuforia^(TM)
    website). To validate your configuration, click on **Add**, and wait as the image
    is being processed. When the processing is over, you should get a screen like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring VuforiaTM to recognize objects](img/8553_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The stars notify you of the quality of the target for tracking. This example
    has five stars, which means it will work really well. You can get more information
    on the Vuforia^(TM) website on how to create a good image for a target: [https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating](https://developer.vuforia.com/resources/dev-guide/natural-features-and-rating).'
  prefs: []
  type: TYPE_NORMAL
- en: Our last step is now to export the created target. So select the target (tick
    the box next to **VuforiaJMETarget**), and click on **Download Selected Targets**.
    On the dialog box that appears, choose **SDK** for export and **VuforiaJME** for
    our database name, and save.
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring VuforiaTM to recognize objects](img/8853_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unzip your compressed file. You will see two files: a `.dat` file and a `.xml`
    file. Both files are used for operating the Vuforia^(TM) tracking at runtime.
    The `.dat` file specifies the feature points from your image and the `.xml` file
    is a configuration file. Sometimes you may want to change the size of your marker
    or do some basic editing without having to restart or do the training; you can
    modify it directly on your XML file. So now we are ready with our target for implementing
    our first Vuforia^(TM) project!'
  prefs: []
  type: TYPE_NORMAL
- en: Putting it together – Vuforia^(TM) with JME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section we will show you how to integrate Vuforia^(TM) with JME. We
    will use a natural feature-tracking target for this purpose. So open the **VuforiaJME**
    project in your Eclipse to start. As you can already observe, there are two main
    changes compared to our previous projects:'
  prefs: []
  type: TYPE_NORMAL
- en: The camera preview class is gone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a new directory in the project root named `jni`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first change is due to the way Vuforia^(TM) manages the camera. Vuforia^(TM)
    uses its own camera handle and camera preview integrated in the library. Therefore,
    we'll need to query the video image through the Vuforia^(TM) library to display
    it on our scene graph (using the same principle as seen in [Chapter 2](ch02.html
    "Chapter 2. Viewing the World"), *Viewing the World*).
  prefs: []
  type: TYPE_NORMAL
- en: The `jni` folder contains C++ source code, which is required for Vuforia^(TM).
    To integrate Vuforia^(TM) with JME, we need to interoperate Vuforia's low-level
    part (C++) with the high-level part (Java). It means we will need to compile C++
    and Java code and transfer data between them. If you have done it, you'll need
    to download and install the Android NDK before going further (as explained in
    [Chapter 1](ch01.html "Chapter 1. Augmented Reality Concepts and Tools"), *Augmented
    Reality Concepts and Tools*).
  prefs: []
  type: TYPE_NORMAL
- en: The C++ integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The C++ layer is based on a modified version of the **ImageTargets** example
    available on the Vuforia^(TM) website. The `jni` folder contains the following
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MathUtils.cpp` and `MathUtils.h`: Utilities functions for mathematical computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VuforiaNative.cpp`: This is the main C++ class that interacts with our Java
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Android.mk` and `Application.mk`: These contains configuration files for compilation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the `Android.mk` file, and check if the path to your Vuforia^(TM) installation
    is correct in the `QCAR_DIR` directory. Use only a relative path to make it cross-platform
    (on MacOS with the android ndk r9 or higher, an absolute path will be concatenated
    with the current directory and result in an incorrect directory path).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now open the `VuforiNative.cpp` file. A lot of functions are defined in the
    files but only three are relevant to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Java_com_ar4android_VuforiaJMEActivity_loadTrackerData(JNIEnv *, jobject)`:
    This is the function for loading our specific target (created in the previous
    section)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`virtual void QCAR_onUpdate(QCAR::State& state)`: This is the function to query
    the camera image and transfer it to the Java layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Java_com_ar4android_VuforiaJME_updateTracking(JNIEnv *env, jobject obj)`:
    This function is used to query the position of the targets and transfer it to
    the Java layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first step will be to use our specific target in our application and the
    first function. So copy and paste the `VuforiaJME.dat` and `VuforiaJME.xml` files
    to your assets directory (there should already be two target configurations).
    Vuforia^(TM) configures the target that will be used based on the XMLconfiguration
    file. `loadTrackerData` gets first access to `TrackerManager` and `imageTracker`
    (which is a tracker for non-natural features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create a specific target, such as instancing a dataset.
    In this example, one dataset is created, named `dataSetStonesAndChips`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After we load the configuration of the targets in the created instance, this
    is where we set up our VuforiaJME target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally we can activate the dataset by calling the `activateDataSet` function.
    If you don''t activate the dataset, the target will be loaded and initialized
    in the tracker but won''t be tracked until activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our target initialized, we need to get the real view of the world
    with Vuforia^(TM). The concept is the same as we have seen before: using a video
    background camera in the JME class and updating it with an image. However, here,
    the image is not coming from a Java `Camera.PreviewCallback` but from Vuforia^(TM).
    In Vuforia^(TM) the best place to get the video image is in the `QCAR_onUpdate`
    function. This function is called just after the tracker gets updated. An image
    can be retrieved by querying a frame on the State object of Vuforia^(TM) with
    `getFrame()`. A frame can contain multiple images, as the camera image is in different
    formats (for example, YUV, RGB888, GREYSCALE, RGB565, and so on). In the previous
    example, we used the RGB565 format in our JME class. We will do the same here.
    So our class will start as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The function parses a list of images in the frame and retrieves the `RGB565`
    image. Once we get this image, we need to transfer it to the **Java Layer**. For
    doing that you can use a JNI function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we get information about the size of the image and a pointer
    on the raw data of the image. We use a JNI function named `setRGB565CameraImage`,
    which is defined in our `Java Activity` class. We call this function from C++
    and pass in argument the content of the image (`pixelArray`) as `width` and `height`
    of the image. So each time the tracker updates, we retrieve a new camera image
    and send it to the Java layer by calling the `setRGB565CameraImage` function.
    The JNI mechanism is really useful and you can use it for passing any data, from
    a sophisticated computation process back to your Java class (for example, physics,
    numerical simulation, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to retrieve the location of the targets from the tracking.
    We will do that from the `updateTracking` function. As before, we get an instance
    of the State object from Vuforia^(TM). The State object contains `TrackableResults`,
    which is a list of the identified targets in the video image (identified here
    as being recognized as a target and their position identified):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In our example, we have only one target activated, so if we get a result, it
    will obviously be our marker. We can then directly query the position information
    from it. If you had multiple activated markers, you will need to identify which
    one is which, by getting information from the result by calling `result->getTrackable()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The position of `trackable` is queried by calling `result->getPose()`, which
    returns a matrix defining a linear transformation. This transformation gives you
    the position of the marker relative to the camera position. Vuforia^(TM) uses
    a computer-vision coordinate system (x on the left, y down, and z away from you),
    which is different from JME, so we will have to do some conversion later on. For
    now, what we will do first is inverse the transformation, to get the position
    of the camera relative to the marker; this will make the marker the reference
    coordinate system for our virtual content. So you will do some basic mathematical
    operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the location (`cam_x,y,z`) as well as the orientation of our camera
    `(cam_right_/cam_up_/cam_dir_x,y,z`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to transfer this information to the Java layer. We will use
    JNI again for this operation. What we also need is information about the internal
    parameters of our camera. This is similar to what was discussed in [Chapter 3](ch03.html
    "Chapter 3. Superimposing the World"), *Superimposing the World*, but now it has
    been done here with Vuforia^(TM). For that, you can access the `CameraCalibration`
    object from `CameraDevice`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily transform the projection transformation to a more readable format
    for the camera configuration, such as its field of view (`fovDegrees`), which
    we also have to adapt to allow for differences in the aspect ratios of the camera
    sensor and the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call three JNI functions to transfer the field of view (`setCameraPerspectiveNative`),
    camera position (`setCameraPoseNative`) and camera orientation (`setCameraOrientationNative`)
    to our Java layer. These three functions are time defined in the `VuforiaJME`
    class, which allows us to quickly modify our virtual camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step will be to compile the program. So run a command shell, and go
    the `jni` directory containing the files. From there you need to call the `ndk-build`
    function. The function is defined in your `android-ndk-r9d` directory, so be sure
    it''s accessible from your path. If everything goes well, you should see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Time to go back to Java!
  prefs: []
  type: TYPE_NORMAL
- en: The Java integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Java layer defines the function previously called using similar classes
    from our *Superimpose* example. The first function is the `setRGB565CameraImage`
    function which handles the video image as in the previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: The other JNI functions will modify the characteristics of our foreground camera.
    Specifically, we modify the left axis of the JME camera to match the coordinate
    system used by Vuforia^(TM) (as depicted in the figure in the *Choosing physical
    objects* section).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have to adjust the viewport of the background camera, which shows
    the camera image, to prevent 3D objects from floating above the physical target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it. What we want to outline again here is the concept behind it:'
  prefs: []
  type: TYPE_NORMAL
- en: The camera model used in your tracker is matched with your virtual camera (in
    this example `CameraCalibration` from Vuforia^(TM) to our JME Virtual Camera).
    This will guarantee us a correct registration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You track a target in your camera coordinate system (in this example, a natural
    feature target from Vuforia^(TM)). This tracking replaces our GPS as seen previously,
    and uses a local coordinate system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The position of this target is used to modify the pose of your virtual camera
    (in this example, transferring the detected position from C++ to Java with JNI,
    and updating our JME Virtual Camera). As we repeat the process for each frame,
    we have a full 6DOF registration between physical (the target) and virtual (our
    JME scene).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your results should look similar to the one in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Java integration](img/8553_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to computer vision-based AR. We developed
    an application with the Vuforia^(TM) library and showed you how to integrate it
    with JME. You are now ready to create natural feature tracking-based AR applications.
    In this demo, you can move your device around the marker and see the virtual content
    from every direction. In the next chapter, we will learn how we can do more in
    terms of interaction. How about being able to select the model and play with it?
  prefs: []
  type: TYPE_NORMAL
