- en: Chapter 3. Superimposing the World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have a view of the physical world on your screen, our next goal
    is to overlay digital 3D models on top of it. Overlay in 3D as used in Augmented
    Reality, is different from basic 2D overlays possible with Adobe Photoshop or
    similar drawing applications (in which we only adjust the position of two 2D layers).
    The notion of 3D overlay involves the management and rendering of content with
    six degrees of freedom (translation and rotation in three dimensions) as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Superimposing the World](img/8553_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will guide you through the different concepts and present
    you with the best way to superimpose real and virtual content. We will successively
    describe the concept of real and virtual cameras, how to perform superimposition
    with our scene graph engine, and create high quality superimposition. First, let's
    discuss the 3D world and the virtual camera.
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks of 3D rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representing and rendering virtual 3D content operates in the same way as when
    you click a picture with a digital camera in the physical world. If you take a
    picture of your friend or a landscape, you will first check your subject with
    the naked eye and after that will look at it through the viewfinder of the camera;
    only then will you take the picture. These three different steps are the same
    with virtual 3D content. You do not have a physical camera taking pictures, but
    you will use a **virtual camera** to render your scene. Your virtual camera can
    be seen as a digital representation of a real camera and can be configured in
    a similar way; you can position your camera, change its field of view, and so
    on. With virtual 3D content, you manipulate a digital representation of a geometrical
    3D scene, which we simply call your virtual 3D scene or virtual world.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three basic steps for rendering a scene using 3D computer graphics are
    shown in the following figure and consist of:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your virtual 3D scene (objects position and appearance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring your virtual camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rendering the 3D scene with the virtual camera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The building blocks of 3D rendering](img/8553_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we do real-time rendering for AR, you will repeat these steps in a loop;
    objects or cameras can be moved at each time frame (typically at 20-30 FPS).
  prefs: []
  type: TYPE_NORMAL
- en: While positioning objects in a scene, or the camera in a scene, we need a way
    of representing the location (and also the orientation) of objects as functions
    of each other. To do so, we generally use some spatial representation of the scene
    based on geometric mathematical models. The most common approach is to use **Euclidian
    geometry** and **coordinate systems**. A coordinate system defines a method of
    referencing an object (or point) in a space using a numerical representation to
    define this position (**coordinates**). Everything in your scene can be defined
    in a coordinate system, and coordinate systems can be related to each other using
    **transformations**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common coordinate systems are shown in the following figure and are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**World Coordinate System**: It is the ground where you reference everything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera Coordinate System**: It is placed in the world coordinate system and
    used to render your scene seen from this specific viewpoint. It is sometimes also
    referenced as the Eye Coordinate System.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local Coordinate System(s)**: It is, for example, an object coordinate system,
    used to represent the 3D points of an object. Traditionally, you use the (geometric)
    center of your object to define your local coordinate system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The building blocks of 3D rendering](img/8553_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two conventions for the orientation of the coordinate systems: left-handed
    and right-handed. In both the conventions, X goes on the right-hand side and Y
    goes upwards. Z goes towards you in the right-handed convention and away from
    you in the left-handed convention.'
  prefs: []
  type: TYPE_NORMAL
- en: Another common coordinate system, not illustrated here, is the image coordinate
    system. You are probably familiar with this one if you edit your pictures. It
    defines the position of each pixel of your image from a referenced origin (commonly
    the top-left corner or the bottom-left corner of an image). When you perform 3D
    graphics rendering, it's the same concept. Now we will focus on the virtual camera
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Real camera and virtual camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A virtual camera for 3D graphics rendering is generally represented by two
    main sets of parameters: the **extrinsic** and **intrinsic** parameters. The extrinsic
    parameters define the location of the camera in the virtual world (the transformation
    from the world coordinate system to the camera coordinate system and vice versa).
    The intrinsic parameters define the projective properties of the camera, including
    its field of view (focal length), image center, and skew. Both the parameters
    can be represented with different data structures, with the most common being
    a matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you develop a 3D mobile game, you are generally free to configure the cameras
    the way you want; you can put the camera above a 3D character running on a terrain
    (extrinsic) or set up a large field of view to have a large view of the character
    and the terrain (intrinsic). However, when you do Augmented Reality, the choice
    is constrained by the properties of the real camera in your mobile phone. In AR,
    we want properties of the virtual camera to match those of the real camera: the
    field of view and the camera position. This is an important element of AR, and
    we will explain how to realize it further in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Camera parameters (intrinsic orientation)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The extrinsic parameters of the virtual camera will be explored in subsequent
    chapters; they are used for 3D registration in Augmented Reality. For our 3D overlay,
    we will now explore the intrinsic camera parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different computational models for representing a virtual camera
    (and its parameters) and we will use the most popular one: the pinhole camera
    model. The pinhole camera model is a simplified model of a physical camera, where
    you consider that there is only a single point (pinhole) where light enters your
    camera image. With this assumption, computer vision researchers simplify the description
    of the intrinsic parameters as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Focal length of your (physical or virtual) lens**: This together with the
    size of the camera center determines the **field of view** (**FOV**)—also called
    the angle of view—of your camera. The FOV is the extent of the object space your
    camera can see and is represented in radians (or degrees). It can be determined
    for the horizontal, vertical, and diagonal direction of your camera sensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image center (principal point)**: This accommodates any displacement of the
    sensor from the center position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skew factor**: This is used for non-square pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On non-mobile cameras you should also consider the lens distortion, such as
    the radial and the tangential distortions. They can be modeled and corrected with
    advanced software algorithms. Lens distortions on mobile phone cameras are usually
    corrected in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: With all these concepts in mind, let's do a bit of practice now.
  prefs: []
  type: TYPE_NORMAL
- en: Using the scenegraph to overlay a 3D model onto the camera view
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter you learned how to set up a single viewport and camera
    to render the video background. While the virtual camera determines how your 3D
    graphics are projected on a 2D image plane, the viewport defines the mapping of
    this image plane to a part of the actual window in which your application runs
    (or the whole screen of the smartphone if the app runs in fullscreen mode). It
    determines the portion of the application window in which graphics are rendered.
    Multiple viewports can be stacked and can cover the same or different screen areas
    as shown in the following figure. For a basic AR application, you typically have
    two viewports. One is associated with the camera rendering the background video
    and one is used with a camera rendering the 3D objects. Typically, these viewports
    cover the whole screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the scenegraph to overlay a 3D model onto the camera view](img/8553_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The viewport size is not defined in pixels but is unitless and is defined from
    0 to 1 for the width and height to be able to easily adapt to changing window
    sizes. One camera is associated with one viewport at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that for the video background we used an orthographic camera to avoid
    perspective foreshortening of the video image. However, this perspective is crucial
    for getting a proper visual impression of your 3D objects. Orthographic (parallel)
    projection (on the left-hand side of the following figure) and perspective projection
    (on the right-hand side of the following figure) determine how the 3D volume is
    projected on a 2D image plane as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the scenegraph to overlay a 3D model onto the camera view](img/8553_03_05a_FINAL.jpg)'
  prefs: []
  type: TYPE_IMG
- en: JME uses a right-handed coordinate system (OpenGL® convention, x on the right-hand
    side, y upwards, and z towards you). You certainly want 3D objects to appear bigger
    as the camera moves closer to them and smaller as it moves away. So how do we
    go along? Right, you just add a second camera—this time a perspective one—and
    an associated viewport that also covers the whole application window.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `SuperimposeJME` project associated with this chapter, we again have
    Android activity (`SuperimposeJMEActivity.java`) and a JME application class (`SuperimposeJME.java`).
    The application needs no major change from our previous project; you only have
    to extend the JME `SimpleApplication` class. In its `simpleInitApp()` startup
    method, we now explicitly differentiate between the initialization of the scene
    geometry (video background: `initVideoBackground()`; 3D foreground scene: `initForegroundScene()`)
    and the associated cameras and viewports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the order in which the camera and viewports are initialized is important.
    Only when we first add the camera and viewport for the video background (`initBackgroundCamera()`)
    and later add the foreground camera and viewport (`initForegroundCamera()`), can
    we ensure that our 3D objects are rendered on top of the video background; otherwise,
    you would only see the video background.
  prefs: []
  type: TYPE_NORMAL
- en: We will now add your first 3D model into the scene using `initForegroundScene()`.
    A convenient feature of JME is that it supports the loading of external assets—for
    example, Wavefront files (`.obj`) or Ogre3D files (.`mesh.xml`/`.scene`)—including
    animations. We will load and animate a green ninja, a default asset that ships
    with JME.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So in this method you load a model relative to your project's `root`/`asset`
    folder. If you want to load other models, you also place them in this `asset`
    folder. You scale, translate, and orient it and then add it to the root scenegraph
    node. To make the model visible, you also add a directional light shining from
    the top front onto the model (you can try not adding the light and see the result).
    For the animation, access the "Walk" animation sequence stored in the model. In
    order to do this, your class needs to implement the `AnimEventListener` interface
    and you need to use an `AnimControl` instance to access that animation sequence
    in the model. Finally, you will assign the "Walk" sequence to an `AnimChannel`
    instance, tell it to loop the animation, and set the animation speed.
  prefs: []
  type: TYPE_NORMAL
- en: Great, you have now loaded your first 3D model, but you still need to display
    it on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: This is what you do next in `initForegroundCamera(fovY)`. It takes care of setting
    up the perspective camera and the associated viewport for your 3D model. As the
    perspective camera is characterized by the spatial extent of the object space
    it can see (the FOV), we pass the vertical angle of view stored in `mForegroundCamFOVY`
    to the method.It then attaches the root node of our scene containing the 3D model
    to the foreground viewport.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: While you could just copy some standard parameters from the default camera (similar
    to what we did with the video background camera), it is good to know which steps
    you actually have to do to initialize a new camera. After creating a perspective
    camera initialized with the window width and height, you set both the location
    (`setLocation()`) and the rotation (`setAxes()`) of the camera. JME uses a right-handed
    coordinate system, and our camera is configured to look along the negative z axis
    into the origin just as depicted in the previous figure. In addition, we set the
    vertical angle of the view passed to `setFrustumPerspective()` to 30 degrees,
    which corresponds approximately with a field of view that appears natural to a
    human (as opposed to a very wide or very narrow field of view).
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterwards, we set up the viewport as we did for the video background camera.
    In addition, we tell the viewport to delete its depth buffer but retain the color
    and stencil buffers with `setClearFlags(false, true, false)`. We do this to ensure
    that our 3D models are always rendered in front of the quadrilateral holding the
    video texture, no matter if they are actually before or behind that quad in object
    space (beware that all our graphical objects are referenced in the same world
    coordinate system). We do not clear the color buffer as, otherwise, the color
    values of the video background, which are previously rendered into the color buffer
    will be deleted and we will only see the background color of this viewport (blue).
    If you run your application now, you should be able to see a walking ninja in
    front of your video background, as shown in the following pretty cool screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the scenegraph to overlay a 3D model onto the camera view](img/8553_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Improving the overlay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section you created a perspective camera, which renders your
    model with a vertical field of view of 30 degrees. However, to increase the realism
    of your scene, you actually want to match the field of view of your virtual and
    physical cameras as well as possible. This field of view in a general imaging
    system such as your phone's camera is dependent both on the size of the camera
    sensor and the focal length of the optics used. The focal length is a measure
    of how strongly the camera lens bends incoming parallel light rays until they
    come into focus (on the sensor plane), it is basically the distance between the
    sensor plane and the optical elements of your lens.
  prefs: []
  type: TYPE_NORMAL
- en: The FOV can be computed from the formula *α = 2 arctan d/2f*, where *d* is the
    (vertical, horizontal, or diagonal) extent of the camera sensor and *2* is the
    focal length. Sounds easy, right? There is only a small challenge. You most often
    do not know the (physical) sensor size or the focal length of the phone camera.
    The good thing about the preceding formula is that you do not need to know the
    physical extent of your sensor or its focal length but can calculate it in arbitrary
    coordinates such as pixels. And for the sensor size, we can easily use the resolution
    of the camera image, which you already learned to query in [Chapter 2](ch02.html
    "Chapter 2. Viewing the World"), *Viewing the World*.
  prefs: []
  type: TYPE_NORMAL
- en: The trickiest part is to estimate the focal length of your camera. There are
    some tools that help you to do just this using a set of pictures taken from a
    known object; they are called camera resectioning tools (or geometric camera calibration
    tools). We will show you how to achieve this with a tool called GML C++ Camera
    Calibration Toolbox, which you can download from [http://graphics.cs.msu.ru/en/node/909](http://graphics.cs.msu.ru/en/node/909).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the tool, open the standard camera app on your Android phone.
    Under the still image settings select the camera resolution that you also use
    in your JME application, for example, **640 x 480**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving the overlay](img/8553_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Take an A4 size printout of the `checkerboard_8x5_A4.pdf` file in the GML Calibration
    pattern subdirectory. Take at least four pictures with your camera app from different
    viewpoints (6 to 8 pictures will be better). Try to avoid very acute angles and
    try to maximize the checkerboards in the image. Example images are depicted in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving the overlay](img/8553_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When you are done, transfer the images to a folder on your computer (for example,
    `AR4Android\calibration-images`). Afterwards, start the GML Camera Calibration
    app on your computer and create a new project. Type into the **New project** dialog
    box the correct number of black and white squares (for example, `5` and `8`),
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving the overlay](img/8553_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is also crucial to actually measure the square size as your printer might
    scale the PDF to its paper size. Then, click on **OK** and start adding the pictures
    you have just taken (navigate to **Object detection** | **Add image**). When you
    have added all the images, navigate to **Object detection** | **Detect All** and
    then **Calibration** | **Calibrate**. If the calibration was successful, you should
    see camera parameters in the result tab. We are mostly interested in the **Focal
    length** section. While there are two different focal lengths for the x and y
    axes, it is fine to just use the first one. In the sample case of the images,
    which were taken with a Samsung Galaxy SII, the resulting focal length is 522
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then plug this number together with your vertical image resolution
    into the preceding formula and retrieve the vertical angle of the view in radians.
    As JME needs the angle in degrees, you simply convert it by applying this factor:
    *180/PI*. If you are also using a Samsung Galaxy SII, a vertical angle of view
    of approximately 50 degrees should result, which equals a focal length of approximately
    28 mm in 35 mm film format (wide angle lens). If you plug this into the `mForegroundCamFOVY`
    variable and upload the application, the walking ninja should appear smaller as
    shown in the following figure. Of course, you can increase its size again by adjusting
    the camera position.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that you cannot model all parameters of the physical camera in JME. For
    example, you cannot easily set the principal point of your physical camera with
    your JME camera.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JME also doesn''t support direct lens distortion correction. You can account
    for these artifacts via advanced lens correction techniques covered, for example,
    here: [http://paulbourke.net/miscellaneous/lenscorrection/](http://paulbourke.net/miscellaneous/lenscorrection/).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving the overlay](img/8553_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced you to the concept of 3D rendering, the 3D virtual
    camera, and the notion of 3D overlay for Augmented Reality. We presented what
    a virtual camera is and its characteristics and described the importance of intrinsic
    camera parameters for accurate Augmented Reality. You also got a chance to develop
    your first 3D overlay and calibrate your mobile camera for improved realism. However,
    as you move your phone along, the video background changes, while the 3D models
    stay in place. In the next chapter, we will tackle one of the fundamental bricks
    of an Augmented Reality application: the registration.'
  prefs: []
  type: TYPE_NORMAL
