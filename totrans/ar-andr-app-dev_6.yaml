- en: Chapter 6. Make It Interactive – Create the User Experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the course of the previous chapters, we''ve learned the essentials of
    creating augmentations using the two most common AR approaches: sensor-based and
    computer vision-based AR. We are now able to overlay digital content over a view
    of the physical world, support AR tracking, and handle account registration (on
    a target or outdoors).'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can merely navigate the augmented world around them. Wouldn''t
    it be cool to allow the users to also interact with the virtual content in an
    intuitive way? User interaction is a major component in the development of any
    application. As we are focusing here on the user interaction with 3D content (3D
    interaction), the following are three main categories of interaction techniques
    that can be developed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Navigation**: Moving around a scene and selecting a specific viewpoint. In
    AR, this navigation is done by physical motion (such as, walking on the street
    or turning a table) and can be complemented with additional virtual functions
    (for example, map view, navigation path, freeze mode, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manipulation**: Selecting, moving, and modifying objects. In AR, this can
    be done on physical and virtual elements, through a range of traditional methods
    (for example, ray picking), and novel interaction paradigms (for example, tangible
    user interfaces).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System control**: Adapting the parameters of your application, including
    rendering, polling processes, and application dependent content. In AR, it can
    correspond to adjusting tracking parameters or visualization techniques (for example,
    presenting the distance to your POI in an AR Browser).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter we will show you a subset of some of the commonly used AR interaction
    techniques. We will show you how to develop three interaction techniques, including
    ray picking, proximity-based interaction, and 3D motion gesture-based interaction.
    This is the next step in designing an AR Application and a fundamental brick in
    our AR layer (See [Chapter 1](ch01.html "Chapter 1. Augmented Reality Concepts
    and Tools"), *Augmented Reality Concepts and Tools*).
  prefs: []
  type: TYPE_NORMAL
- en: Pick the stick – 3D selection using ray picking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3D interaction on desktop computers made use of a limited set of devices, including
    the keyboard, mouse, or joystick (for games). On a smartphone (or tablet), interaction
    is mainly driven by touch or sensor input. From an interaction input (the sensor
    data, such as x and y coordinates on the screen, or the event type, such as click
    or dwell), you can develop different interaction techniques, such as ray picking,
    steering navigation, and so on. For mobile AR, a large set of interaction techniques
    can be used for 2D or 3D interactions. In this section, we will look at using
    touch input combined with a technique named **ray picking**.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of ray picking is to use a virtual ray going from your device to
    your environment (which is the target) and detect what it hits along the way.
    When you get a hit on some object (for example, the ray intersects with one of
    your virtual characters), you can consider this object picked (selected) and start
    to manipulate it. Here, we will only look at how we can pick an object in JME.
    In the sample code, you can extend the object to support further manipulation,
    for example, when an object is hit and picked, you can detect sliding touch motion
    and translate the object, make it explode, or use the hit as a shooting ray for
    some game, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So let's start. In JME, you can use either an Android-specific function for
    the input (via `AndroidInput`) or the same one you would use in a desktop application
    (`MouseInput`). JME on Android, by default, maps any touch event as a mouse event
    that allows us to have (almost) the same code working on Android and your desktop.
    We will choose the following solution for this project; as an exercise, you can
    try to use `AndroidInput` (look into `AndroidTouchInputListener` to use `AndroidInput`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `RayPickingJME` example. It''s using the same base code as `VuforiaJME`
    and our picking method is based on a JME example, for this picking method please
    visit the following link: [http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking](http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is add the different packages necessary for ray picking
    in our `RayPickingJME` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To be able to pick an object, we need to declare some global variables in the
    scope of our `RayPicking` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Node shootables`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Geometry geom`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next step is to add a listener to our class. If you''ve never done Android
    or JME programming, you may not know what a listener is. A **listener** is an
    event handler technique that can listen to any of the activities happening in
    a class and provide specific methods to handle any event. For example, if you
    have a mouse button click event, you can create a listener for it that has an
    `onPushEvent()` method where you can install your own code. In JME, event management
    and listeners are organized into two components, controlled by using the `InputManager`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trigger mapping**: Using this you can associate the device input can with
    a trigger name, for example, clicking on the mouse can be associated with `Press`
    or `Shoot` or `MoveEntity`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listener**: Using this you can associate the trigger name with a specific
    listener; either `ActionListener` (used for discrete events, such as "button pressed")
    or `AnalogListener` (used for continuous events, such as the amplitude of a joystick
    movement).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, in your `simpleInitApp` procedure, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, here, we map the occasions when the spacebar is pressed (even when using
    a virtual keyboard) and mouse click (which is a touch action on our mobile) to
    the trigger name `Shoot`. This trigger name is associated with the `ActionListener`
    event listener that we've named `actionListener`. The action listener will be
    where we do the ray picking; so, on a touchscreen device, by touching the screen,
    you can activate `actionListener` (using the trigger `Shoot`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step is to define the list of objects that can potentially be hit
    by our ray picking. A good technique for that is to regroup them under a specific
    group node. In the following code, we will create a box object and place it under
    a group node named `shootables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we have our touch mapping and our objects that can be hit. We only need
    to implement our listener. The way to ray cast in JME is similar to that in many
    other libraries; we use the hit coordinates (defined in the screen coordinates),
    transform them using our camera, create a ray, and run a hit. In our AR example,
    we will use the AR camera, which is updated by our computer vision-based tracker
    `fgCam`. So, the code is the same in AR as for another virtual game, except that
    here, our camera position is updated by the tracker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a `Ray` object and run a picking test (hitting test) by calling `collideWith`
    for our list object that can be hit (`shootables`). Results of the collision will
    be stored in a `CollisionResults` object. So, our listener''s code looks like
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what do we do with the result? As explained earlier in the book, you can
    manipulate it in a different way. We will do something simple here; we will detect
    whether or not our box is selected, and if it is, change its color to red for
    no intersection and green if there is an intersection. We first print the results
    for debugging, where you can use the `getCollision()` function to detect which
    object has been hit (`getGeometry()`), at what distance (`getDistance()`), and
    the point of contact (`getContactPoint()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So, by using the preceding code we can detect whether or not we have any result,
    and since we only have one object in our scene, we consider that if we got a hit,
    it''s our object, so we change the color of the object to green. If we don''t
    get any hit, since there is only our object, we turn it red:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a result similar to that shown in the following screenshot (hit:
    left, miss: right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pick the stick – 3D selection using ray picking](img/8553OS_06_01a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can now deploy and run the example; touch the object on the screen and see
    our box changing color!
  prefs: []
  type: TYPE_NORMAL
- en: Proximity-based interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another type of interaction in AR is using the relation between the camera
    and a physical object. If you have a target placed on a table and you move around
    with your device to look at a virtual object from different angles, you can also
    use that to create interaction. The idea is simple: you can detect any change
    in spatial transformation between your camera (on your moving device) and your
    target (placed on a table), and trigger some events. For example, you can detect
    if the camera is under a specific angle, if it''s looking at the target from above,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will implement a **proximity** technique that can be used
    to create creating some cool animation and effects. The proximity technique uses
    the distance between the AR camera and a computer vision-based target.
  prefs: []
  type: TYPE_NORMAL
- en: So, open the `ProximityBasedJME` project in your Eclipse. Again, this project
    is also based on the `VuforiaJME` example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create three objects—a box, a sphere, and a torus—using three different
    colors—red, green and blue—as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In a large number of scene graph libraries, you will often find a switch node
    that allows the representation of an object based on some parameters to be switched,
    such as the distance from the object to the camera. JME doesn't have a switch
    node, so we will simulate its behavior. We will change which object is displayed
    (box, sphere, or torus) as a function of its distance from the camera. The simple
    way to do that is to add or remove objects that shouldn't be displayed at a certain
    distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the proximity technique, we query the location of our AR camera
    (`fgCam.getLocation()`). From that location, you can compute the distance to some
    objects or just the target. The distance to the target is, by definition, similar
    to the distance of the location (expressed as a vector with three dimensions)
    of the camera. So, what we do is define three ranges for our object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Camera distance 50 and more**: Shows the cube'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera distance 40-50**: Shows the sphere'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera distance under 40**: Shows the torus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The resulting code in the `simpleUpdate` method is rather simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run your example and change the distance of the device to that of the tracking
    target. This will affect the object which is presented. A cube will appear when
    you are far away (as shown on the left-hand side of the following figure), torus
    when you are close (as shown on the right-hand side of the following figure),
    and a sphere in between (as shown in the center of the following figure):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Proximity-based interaction](img/553OS_06_02a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simple gesture recognition using accelerometers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4,](ch04.html "Chapter 4. Locating in the World") *Locating in
    the World*, you were introduced to the various sensors that are built into the
    typical Android device. You learned how to use them to derive the orientation
    of your device. However, there is much more you can do with those sensors, specifically
    accelerometers. If you have ever played Wii games, you were surely fascinated
    by the natural interaction you could achieve by waving the Wiimote around (for
    example, when playing a tennis or golf Wii game). Interestingly, the Wiimote uses
    similar accelerometers to many Android smartphones, so you can actually implement
    similar interaction methods as with the Wiimote. For complex 3D-motion gestures
    (such as drawing a figure eight in the air), you will need either some machine
    learning background or access to use libraries such as the one at the following
    link: [http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html](http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html).
    However, if you only want to recognize simple gestures, you can do that easily
    in a few lines of code. Next, we will show you how to recognize simple gestures
    such as a shake gesture, that is, quickly waving your phone back and forth several
    times.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a look at the sample project `ShakeItJME`, you will notice that
    it is, to a large degree, identical to the `SensorFusionJME` project from [Chapter
    4](ch04.html "Chapter 4. Locating in the World"), *Locating in the World*. Indeed,
    we only need to perform a few simple steps to extend any application that already
    uses accelerometers. In `ShakeItJMEActivity`, you first add some variables that
    are relevant for the shake detection, including mainly variables for storing timestamps
    of accelerometer events (`mTimeOfLastShake`, `mTimeOfLastForce`, and `mLastTime`),
    ones for storing past accelerometer forces (`mLastAccelValX`, `mLastAccelValY`,
    and `mLastAccelValZ`), and a number of thresholds for shake durations, timeouts
    (`SHAKE_DURATION_THRESHOLD`, `TIME_BETWEEN_ACCEL_EVENTS_THRESHOLD`, and `SHAKE_TIMEOUT`),
    and a minimum number of accelerometer forces and sensor samples (`ACCEL_FORCE_THRESHOLD`
    and `ACCEL_EVENT_COUNT_THRESHOLD`).
  prefs: []
  type: TYPE_NORMAL
- en: Next, you simply add a call to the `detectShake()` method in your `SensorEventListener::onSensorChanged`
    method in the `Sensor.TYPE_ACCELEROMETER` section of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `detectShake()` method is at the core of your shake detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this method, you basically check whether or not accelerometer values in
    a certain time frame are greater than the threshold value. If they are, you call
    the `onShake()` method of your JME app and integrate the event into your application
    `logic.Note` that, in this example, we only use the accelerometer values in the
    z direction, that is, parallel to the direction in which the camera is pointing.
    You can easily extend this to also include sideways shake movements by incorporating
    the x and y values of the accelerometer in the computation of `curAccForce`. As
    an example of how to trigger events using shake detection, in the `onShake()`
    method of your JME application, we trigger a new animation of our walking ninja:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To avoid that the ninja now spins all the time; we will switch to the walking
    animation after the spin animation has ended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you start your app now and shake the device along the viewing direction,
    you should see how the ninja stops walking and makes a gentle spin, just as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple gesture recognition using accelerometers](img/8553OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've introduced you to three interaction techniques, suitable
    for a wide variety of AR applications. Picking allows you to select 3D objects
    by touching the screen, just like you would in 2D selection. Proximity-based camera
    techniques allow you to experiment with the distance and orientation of your device
    to trigger application events. Finally, we've showed you a simple example of a
    3D gesture detection method to add even more interaction possibilities into your
    application. These techniques should serve as building blocks for you to create
    your own interaction methods, targeted to your specific application scenario.
    In the final chapter, we will introduce some advanced techniques and further reading
    to help you get the best out of your Augmented Reality applications.
  prefs: []
  type: TYPE_NORMAL
