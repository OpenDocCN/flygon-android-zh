- en: Chapter 6. Make It Interactive – Create the User Experience
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章. 让它互动——创建用户体验
- en: 'Over the course of the previous chapters, we''ve learned the essentials of
    creating augmentations using the two most common AR approaches: sensor-based and
    computer vision-based AR. We are now able to overlay digital content over a view
    of the physical world, support AR tracking, and handle account registration (on
    a target or outdoors).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经学习了使用两种最常见的AR方法创建增强现实的要点：基于传感器和基于计算机视觉的AR。我们现在能够将数字内容叠加在物理世界的视图上，支持AR跟踪，以及处理账户注册（在目标上或户外）。
- en: 'However, we can merely navigate the augmented world around them. Wouldn''t
    it be cool to allow the users to also interact with the virtual content in an
    intuitive way? User interaction is a major component in the development of any
    application. As we are focusing here on the user interaction with 3D content (3D
    interaction), the following are three main categories of interaction techniques
    that can be developed:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仅仅能在增强的世界中导航。如果能让用户以直观的方式与虚拟内容互动，岂不是很好吗？用户互动是任何应用程序开发的重要组成部分。由于我们这里专注于用户与3D内容（3D交互）的互动，以下是三种主要的交互技术类别，可以加以开发：
- en: '**Navigation**: Moving around a scene and selecting a specific viewpoint. In
    AR, this navigation is done by physical motion (such as, walking on the street
    or turning a table) and can be complemented with additional virtual functions
    (for example, map view, navigation path, freeze mode, and so on).'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导航**：在场景中移动并选择一个特定的视角。在增强现实（AR）中，这种导航是通过物理移动来完成的（例如，在街上行走或转动桌子），并且可以辅以额外的虚拟功能（例如，地图视图，导航路径，冻结模式等）。'
- en: '**Manipulation**: Selecting, moving, and modifying objects. In AR, this can
    be done on physical and virtual elements, through a range of traditional methods
    (for example, ray picking), and novel interaction paradigms (for example, tangible
    user interfaces).'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：选择、移动和修改对象。在AR中，这可以应用于物理和虚拟元素，通过一系列传统方法（例如，射线选择），以及新颖的交互范式（例如，有形用户界面）。'
- en: '**System control**: Adapting the parameters of your application, including
    rendering, polling processes, and application dependent content. In AR, it can
    correspond to adjusting tracking parameters or visualization techniques (for example,
    presenting the distance to your POI in an AR Browser).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统控制**：调整应用程序的参数，包括渲染、轮询过程和依赖于应用程序的内容。在AR中，它可以对应于调整跟踪参数或可视化技术（例如，在AR浏览器中显示到您的兴趣点（POI）的距离）。'
- en: In this chapter we will show you a subset of some of the commonly used AR interaction
    techniques. We will show you how to develop three interaction techniques, including
    ray picking, proximity-based interaction, and 3D motion gesture-based interaction.
    This is the next step in designing an AR Application and a fundamental brick in
    our AR layer (See [Chapter 1](ch01.html "Chapter 1. Augmented Reality Concepts
    and Tools"), *Augmented Reality Concepts and Tools*).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示一些常用的AR交互技术的一个子集。我们将向您展示如何开发三种交互技术，包括射线选择、基于接近度的交互和基于3D运动手势的交互。这是设计AR应用程序的下一步，也是我们AR层的基本构建块（请参阅[第1章](ch01.html
    "第1章. 增强现实概念和工具")，*增强现实概念和工具*）。
- en: Pick the stick – 3D selection using ray picking
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拿起棍子——使用射线选择进行3D选择
- en: 3D interaction on desktop computers made use of a limited set of devices, including
    the keyboard, mouse, or joystick (for games). On a smartphone (or tablet), interaction
    is mainly driven by touch or sensor input. From an interaction input (the sensor
    data, such as x and y coordinates on the screen, or the event type, such as click
    or dwell), you can develop different interaction techniques, such as ray picking,
    steering navigation, and so on. For mobile AR, a large set of interaction techniques
    can be used for 2D or 3D interactions. In this section, we will look at using
    touch input combined with a technique named **ray picking**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在台式计算机上，3D交互使用的是一组有限的设备，包括键盘、鼠标或游戏操纵杆。在智能手机（或平板电脑）上，交互主要由触摸或传感器输入驱动。从交互输入（传感器数据，如在屏幕上的x和y坐标，或事件类型，如点击或悬停）开始，您可以开发不同的交互技术，如射线选择、转向导航等。对于移动AR，可以使用大量交互技术进行2D或3D交互。在本节中，我们将探讨结合触摸输入和名为**射线选择**的技术。
- en: The concept of ray picking is to use a virtual ray going from your device to
    your environment (which is the target) and detect what it hits along the way.
    When you get a hit on some object (for example, the ray intersects with one of
    your virtual characters), you can consider this object picked (selected) and start
    to manipulate it. Here, we will only look at how we can pick an object in JME.
    In the sample code, you can extend the object to support further manipulation,
    for example, when an object is hit and picked, you can detect sliding touch motion
    and translate the object, make it explode, or use the hit as a shooting ray for
    some game, and so on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 射线拣选的概念是使用一个从你的设备到你的环境（即目标）的虚拟射线，并检测沿途它击中了什么。当你在某个对象上得到一个击中（例如，射线与你的虚拟角色之一相交），你可以认为这个对象已被拣选（选中）并开始操作它。在这里，我们只看如何在JME中拣选一个对象。在示例代码中，你可以扩展对象以支持进一步的操作，例如，当一个对象被击中并拣选，你可以检测滑动触摸动作并平移对象，让它爆炸，或者将击中用作某些游戏的射击射线，等等。
- en: So let's start. In JME, you can use either an Android-specific function for
    the input (via `AndroidInput`) or the same one you would use in a desktop application
    (`MouseInput`). JME on Android, by default, maps any touch event as a mouse event
    that allows us to have (almost) the same code working on Android and your desktop.
    We will choose the following solution for this project; as an exercise, you can
    try to use `AndroidInput` (look into `AndroidTouchInputListener` to use `AndroidInput`).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们开始。在JME中，你可以使用特定于Android的输入功能（通过`AndroidInput`）或与桌面应用程序相同的输入（`MouseInput`）。默认情况下，JME在Android上将任何触摸事件映射为鼠标事件，这允许我们在Android和桌面几乎使用相同的代码。我们将为这个项目选择以下解决方案；作为一个练习，你可以尝试使用`AndroidInput`（查看`AndroidTouchInputListener`以使用`AndroidInput`）。
- en: 'Open the `RayPickingJME` example. It''s using the same base code as `VuforiaJME`
    and our picking method is based on a JME example, for this picking method please
    visit the following link: [http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking](http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`RayPickingJME`示例。它使用与`VuforiaJME`相同的基代码，我们的拣选方法基于JME的一个示例，对于这种拣选方法，请访问以下链接：[http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking](http://jmonkeyengine.org/wiki/doku.php/jme3:beginner:hello_picking)。
- en: 'The first thing to do is add the different packages necessary for ray picking
    in our `RayPickingJME` class:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的就是在我们的`RayPickingJME`类中添加不同的射线拣选所需的包：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To be able to pick an object, we need to declare some global variables in the
    scope of our `RayPicking` class:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够拣选一个对象，我们需要在`RayPicking`类的范围内声明一些全局变量：
- en: '`Node shootables`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Node shootables`'
- en: '`Geometry geom`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Geometry geom`'
- en: 'The next step is to add a listener to our class. If you''ve never done Android
    or JME programming, you may not know what a listener is. A **listener** is an
    event handler technique that can listen to any of the activities happening in
    a class and provide specific methods to handle any event. For example, if you
    have a mouse button click event, you can create a listener for it that has an
    `onPushEvent()` method where you can install your own code. In JME, event management
    and listeners are organized into two components, controlled by using the `InputManager`
    class:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是向我们的类中添加一个监听器。如果你从未做过Android或JME编程，你可能不知道监听器是什么。**监听器**是一种事件处理技术，可以监听类中发生的任何活动，并提供特定方法来处理任何事件。例如，如果你有一个鼠标按钮点击事件，你可以为它创建一个监听器，该监听器有一个`onPushEvent()`方法，你可以在其中安装你自己的代码。在JME中，事件管理和监听器被组织成两个组件，通过使用`InputManager`类进行控制。
- en: '**Trigger mapping**: Using this you can associate the device input can with
    a trigger name, for example, clicking on the mouse can be associated with `Press`
    or `Shoot` or `MoveEntity`, and so on.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器映射**：使用这个你可以将设备输入与一个触发器名称关联起来，例如，点击鼠标可以与`Press`或`Shoot`或`MoveEntity`等关联。'
- en: '**Listener**: Using this you can associate the trigger name with a specific
    listener; either `ActionListener` (used for discrete events, such as "button pressed")
    or `AnalogListener` (used for continuous events, such as the amplitude of a joystick
    movement).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监听器**：使用这个你可以将触发器名称与特定的监听器关联起来；`ActionListener`（用于离散事件，如“按钮按下”）或`AnalogListener`（用于连续事件，如操纵杆移动的幅度）。'
- en: 'So, in your `simpleInitApp` procedure, add the following code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你的`simpleInitApp`过程中，添加以下代码：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, here, we map the occasions when the spacebar is pressed (even when using
    a virtual keyboard) and mouse click (which is a touch action on our mobile) to
    the trigger name `Shoot`. This trigger name is associated with the `ActionListener`
    event listener that we've named `actionListener`. The action listener will be
    where we do the ray picking; so, on a touchscreen device, by touching the screen,
    you can activate `actionListener` (using the trigger `Shoot`).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里，我们将按下空格键（即使使用虚拟键盘）和鼠标点击（这是在我们移动设备上的触摸动作）映射到触发器名称 `Shoot`。这个触发器名称与名为 `actionListener`
    的 `ActionListener` 事件监听器相关联。动作监听器将是我们进行射线拾取的地方；因此，在触摸屏设备上，通过触摸屏幕，你可以激活 `actionListener`（使用触发器
    `Shoot`）。
- en: 'Our next step is to define the list of objects that can potentially be hit
    by our ray picking. A good technique for that is to regroup them under a specific
    group node. In the following code, we will create a box object and place it under
    a group node named `shootables`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步是定义可能被我们的射线拾取命中的对象列表。一个不错的技术是将它们重新组合在一个特定的组节点下。在下面的代码中，我们将创建一个盒子对象并将其放置在名为
    `shootables` 的组节点下：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we have our touch mapping and our objects that can be hit. We only need
    to implement our listener. The way to ray cast in JME is similar to that in many
    other libraries; we use the hit coordinates (defined in the screen coordinates),
    transform them using our camera, create a ray, and run a hit. In our AR example,
    we will use the AR camera, which is updated by our computer vision-based tracker
    `fgCam`. So, the code is the same in AR as for another virtual game, except that
    here, our camera position is updated by the tracker.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了触摸映射和可以被击中的对象。我们只需要实现我们的监听器。在JME中进行射线投射的方式与许多其他库相似；我们使用击中坐标（在屏幕坐标中定义），通过我们的摄像机进行变换，创建一个射线，并进行击中测试。在我们的AR示例中，我们将使用由基于计算机视觉的追踪器
    `fgCam` 更新的AR摄像机。因此，在AR中的代码与另一个虚拟游戏中的相同，不同之处在于，这里的摄像机位置是由追踪器更新的。
- en: 'We create a `Ray` object and run a picking test (hitting test) by calling `collideWith`
    for our list object that can be hit (`shootables`). Results of the collision will
    be stored in a `CollisionResults` object. So, our listener''s code looks like
    the following code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个 `Ray` 对象并通过调用 `collideWith` 来对我们的可击中对象列表（`shootables`）进行拾取测试（击中测试）。碰撞结果将被存储在一个
    `CollisionResults` 对象中。因此，我们的监听器的代码如下所示：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, what do we do with the result? As explained earlier in the book, you can
    manipulate it in a different way. We will do something simple here; we will detect
    whether or not our box is selected, and if it is, change its color to red for
    no intersection and green if there is an intersection. We first print the results
    for debugging, where you can use the `getCollision()` function to detect which
    object has been hit (`getGeometry()`), at what distance (`getDistance()`), and
    the point of contact (`getContactPoint()`):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该如何处理这个结果呢？正如本书前面所解释的，你可以以不同的方式操作它。这里我们将做一件简单的事情；我们会检测我们的盒子是否被选中，如果被选中，在没有交点的情况下将其颜色变为红色，如果存在交点则变为绿色。我们首先打印结果以便调试，你可以使用
    `getCollision()` 函数来检测哪个对象被击中（`getGeometry()`），在什么距离（`getDistance()`）以及接触点（`getContactPoint()`）：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, by using the preceding code we can detect whether or not we have any result,
    and since we only have one object in our scene, we consider that if we got a hit,
    it''s our object, so we change the color of the object to green. If we don''t
    get any hit, since there is only our object, we turn it red:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用前面的代码我们可以检测是否有任何结果，由于我们的场景中只有一个对象，我们认为如果我们有击中，那就是我们的对象，所以我们把对象的颜色变为绿色。如果我们没有任何击中，因为只有我们的对象，我们将其变为红色：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get a result similar to that shown in the following screenshot (hit:
    left, miss: right):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到一个类似于以下截图所示的结果（击中：左，未击中：右）：
- en: '![Pick the stick – 3D selection using ray picking](img/8553OS_06_01a.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![选择棒子 - 使用射线拾取的3D选择](img/8553OS_06_01a.jpg)'
- en: You can now deploy and run the example; touch the object on the screen and see
    our box changing color!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以部署并运行这个例子；在屏幕上触摸对象，看看我们的盒子颜色变化！
- en: Proximity-based interaction
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于邻近的交互
- en: 'Another type of interaction in AR is using the relation between the camera
    and a physical object. If you have a target placed on a table and you move around
    with your device to look at a virtual object from different angles, you can also
    use that to create interaction. The idea is simple: you can detect any change
    in spatial transformation between your camera (on your moving device) and your
    target (placed on a table), and trigger some events. For example, you can detect
    if the camera is under a specific angle, if it''s looking at the target from above,
    and so on.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AR中的另一种交互方式是利用相机与物理对象之间的关系。如果你在桌子上放置了一个目标，并且你带着设备围绕它移动以从不同角度观察虚拟对象，你也可以使用这种方式来创建交互。这个想法很简单：你可以检测到移动设备上的相机（你的设备）与放在桌子上的目标之间的空间变换的任何变化，并触发一些事件。例如，你可以检测相机是否处于特定角度，是否从上方看向目标，等等。
- en: In this example, we will implement a **proximity** technique that can be used
    to create creating some cool animation and effects. The proximity technique uses
    the distance between the AR camera and a computer vision-based target.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将实现一种**接近性**技术，该技术可用于创建一些酷炫的动画和效果。接近性技术利用了AR相机与基于计算机视觉的目标之间的距离。
- en: So, open the `ProximityBasedJME` project in your Eclipse. Again, this project
    is also based on the `VuforiaJME` example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，请在你的Eclipse中打开`ProximityBasedJME`项目。同样，这个项目也是基于`VuforiaJME`示例的。
- en: 'First, we create three objects—a box, a sphere, and a torus—using three different
    colors—red, green and blue—as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用三种不同的颜色——红色、绿色和蓝色，创建三个对象——一个盒子、一个球体和一个圆环，如下所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In a large number of scene graph libraries, you will often find a switch node
    that allows the representation of an object based on some parameters to be switched,
    such as the distance from the object to the camera. JME doesn't have a switch
    node, so we will simulate its behavior. We will change which object is displayed
    (box, sphere, or torus) as a function of its distance from the camera. The simple
    way to do that is to add or remove objects that shouldn't be displayed at a certain
    distance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多场景图库中，你经常会找到一个允许基于某些参数（如对象到相机的距离）切换对象表示的开关节点。JME没有开关节点，因此我们将模拟其行为。我们将根据物体与相机的距离来改变显示的对象（盒子、球体或圆环）。实现这一点的简单方法是，在特定距离处添加或移除不应该显示的对象。
- en: 'To implement the proximity technique, we query the location of our AR camera
    (`fgCam.getLocation()`). From that location, you can compute the distance to some
    objects or just the target. The distance to the target is, by definition, similar
    to the distance of the location (expressed as a vector with three dimensions)
    of the camera. So, what we do is define three ranges for our object as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现接近性技术，我们查询AR相机（`fgCam.getLocation()`）的位置。从该位置，你可以计算到某些对象或仅是目标的距离。根据定义，到目标的距离类似于相机的位置（用三维向量表示）的距离。所以，我们要做的是为我们的对象定义三个范围，如下所示：
- en: '**Camera distance 50 and more**: Shows the cube'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相机距离50及以上**：显示立方体'
- en: '**Camera distance 40-50**: Shows the sphere'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相机距离40-50**：显示球体'
- en: '**Camera distance under 40**: Shows the torus'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相机距离40以下**：显示圆环'
- en: 'The resulting code in the `simpleUpdate` method is rather simple:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在`simpleUpdate`方法中的生成代码相当简单：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run your example and change the distance of the device to that of the tracking
    target. This will affect the object which is presented. A cube will appear when
    you are far away (as shown on the left-hand side of the following figure), torus
    when you are close (as shown on the right-hand side of the following figure),
    and a sphere in between (as shown in the center of the following figure):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 运行你的示例，并改变设备与追踪目标之间的距离。这将影响所呈现的对象。当你远离时（如下图左侧所示）会出现立方体，靠近时（如下图右侧所示）会出现圆环，在中间距离时（如下图中心所示）会出现球体：
- en: '![Proximity-based interaction](img/553OS_06_02a.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![基于接近性的交互](img/553OS_06_02a.jpg)'
- en: Simple gesture recognition using accelerometers
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用加速度计的简单手势识别
- en: 'In [Chapter 4,](ch04.html "Chapter 4. Locating in the World") *Locating in
    the World*, you were introduced to the various sensors that are built into the
    typical Android device. You learned how to use them to derive the orientation
    of your device. However, there is much more you can do with those sensors, specifically
    accelerometers. If you have ever played Wii games, you were surely fascinated
    by the natural interaction you could achieve by waving the Wiimote around (for
    example, when playing a tennis or golf Wii game). Interestingly, the Wiimote uses
    similar accelerometers to many Android smartphones, so you can actually implement
    similar interaction methods as with the Wiimote. For complex 3D-motion gestures
    (such as drawing a figure eight in the air), you will need either some machine
    learning background or access to use libraries such as the one at the following
    link: [http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html](http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html).
    However, if you only want to recognize simple gestures, you can do that easily
    in a few lines of code. Next, we will show you how to recognize simple gestures
    such as a shake gesture, that is, quickly waving your phone back and forth several
    times.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html "第4章. 在世界中定位")《在世界中定位》中，你已经了解了典型的Android设备内置的各种传感器。你学会了如何使用它们来推导出设备方向。然而，你可以用这些传感器做更多的事情，特别是加速度计。如果你玩过Wii游戏，你肯定会对通过挥动Wiimote实现自然交互而着迷（例如，在玩网球或高尔夫Wii游戏时）。有趣的是，Wiimote使用的加速度计与许多Android智能手机类似，因此你实际上可以实现与Wiimote类似的交互方法。对于复杂的3D运动手势（如在空中画八字），你将需要一些机器学习背景或使用以下链接中的库：[http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html](http://www.dfki.de/~rnessel/tools/gesture_recognition/gesture_recognition.html)。但是，如果你只想识别简单的手势，你可以很容易地在几行代码中实现。接下来，我们将向你展示如何识别简单的手势，比如摇动手势，即快速地前后挥动手机几次。
- en: If you have a look at the sample project `ShakeItJME`, you will notice that
    it is, to a large degree, identical to the `SensorFusionJME` project from [Chapter
    4](ch04.html "Chapter 4. Locating in the World"), *Locating in the World*. Indeed,
    we only need to perform a few simple steps to extend any application that already
    uses accelerometers. In `ShakeItJMEActivity`, you first add some variables that
    are relevant for the shake detection, including mainly variables for storing timestamps
    of accelerometer events (`mTimeOfLastShake`, `mTimeOfLastForce`, and `mLastTime`),
    ones for storing past accelerometer forces (`mLastAccelValX`, `mLastAccelValY`,
    and `mLastAccelValZ`), and a number of thresholds for shake durations, timeouts
    (`SHAKE_DURATION_THRESHOLD`, `TIME_BETWEEN_ACCEL_EVENTS_THRESHOLD`, and `SHAKE_TIMEOUT`),
    and a minimum number of accelerometer forces and sensor samples (`ACCEL_FORCE_THRESHOLD`
    and `ACCEL_EVENT_COUNT_THRESHOLD`).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看示例项目`ShakeItJME`，你会发现它很大程度上与[第4章](ch04.html "第4章. 在世界中定位")《在世界中定位》中的`SensorFusionJME`项目相同。实际上，我们只需执行几个简单的步骤，就可以扩展任何已经使用加速度计的应用程序。在`ShakeItJMEActivity`中，你首先添加一些与摇动检测相关的变量，主要包括存储加速度计事件时间戳的变量（`mTimeOfLastShake`、`mTimeOfLastForce`和`mLastTime`），存储过去的加速度力的变量（`mLastAccelValX`、`mLastAccelValY`和`mLastAccelValZ`），以及用于摇动持续时间、超时（`SHAKE_DURATION_THRESHOLD`、`TIME_BETWEEN_ACCEL_EVENTS_THRESHOLD`和`SHAKE_TIMEOUT`）以及加速度力和传感器样本的最小数量（`ACCEL_FORCE_THRESHOLD`和`ACCEL_EVENT_COUNT_THRESHOLD`）的阈值。
- en: Next, you simply add a call to the `detectShake()` method in your `SensorEventListener::onSensorChanged`
    method in the `Sensor.TYPE_ACCELEROMETER` section of code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你只需在你的`SensorEventListener::onSensorChanged`方法中的`Sensor.TYPE_ACCELEROMETER`代码部分，简单地添加对`detectShake()`方法的调用。
- en: 'The `detectShake()` method is at the core of your shake detection:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`detectShake()`方法是你的摇动检测的核心：'
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this method, you basically check whether or not accelerometer values in
    a certain time frame are greater than the threshold value. If they are, you call
    the `onShake()` method of your JME app and integrate the event into your application
    `logic.Note` that, in this example, we only use the accelerometer values in the
    z direction, that is, parallel to the direction in which the camera is pointing.
    You can easily extend this to also include sideways shake movements by incorporating
    the x and y values of the accelerometer in the computation of `curAccForce`. As
    an example of how to trigger events using shake detection, in the `onShake()`
    method of your JME application, we trigger a new animation of our walking ninja:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，你基本上是检查在特定时间框架内的加速度计数值是否大于阈值。如果是，就调用你的JME应用程序的`onShake()`方法，并将事件整合到你的应用程序`逻辑中。`注意`，在这个例子中，我们只使用了沿z轴的加速度计数值，即与摄像头指向的方向平行。你可以很容易地扩展这一点，也包括侧向摇动运动，通过在`curAccForce`的计算中纳入加速度计的x和y值。以下是如何使用摇动检测来触发事件的示例，在你的JME应用程序的`onShake()`方法中，我们触发了走路忍者的新动画：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To avoid that the ninja now spins all the time; we will switch to the walking
    animation after the spin animation has ended:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免忍者现在一直旋转；我们将在旋转动画结束后切换到行走动画：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you start your app now and shake the device along the viewing direction,
    you should see how the ninja stops walking and makes a gentle spin, just as shown
    in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在启动应用程序并在观看方向上摇动设备，你应该会看到忍者停止行走并做了一个轻柔的旋转，正如下面这幅图所示：
- en: '![Simple gesture recognition using accelerometers](img/8553OS_06_03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![使用加速度计的简单手势识别](img/8553OS_06_03.jpg)'
- en: Summary
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we've introduced you to three interaction techniques, suitable
    for a wide variety of AR applications. Picking allows you to select 3D objects
    by touching the screen, just like you would in 2D selection. Proximity-based camera
    techniques allow you to experiment with the distance and orientation of your device
    to trigger application events. Finally, we've showed you a simple example of a
    3D gesture detection method to add even more interaction possibilities into your
    application. These techniques should serve as building blocks for you to create
    your own interaction methods, targeted to your specific application scenario.
    In the final chapter, we will introduce some advanced techniques and further reading
    to help you get the best out of your Augmented Reality applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们向你介绍了三种交互技术，适用于各种增强现实（AR）应用。选择技术允许你通过触摸屏幕来选择3D对象，就像你在2D选择中所做的那样。基于接近度的摄像头技术允许你通过改变设备的距离和方向来触发应用事件。最后，我们向你展示了一个简单的3D手势检测方法示例，为你的应用程序添加更多交互可能性。这些技术应作为你创建针对特定应用场景的交互方法的构建块。在最后一章，我们将介绍一些高级技术以及进一步阅读材料，帮助你充分利用增强现实应用。 '
