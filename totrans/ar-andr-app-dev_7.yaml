- en: Chapter 7. Further Reading and Tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we will present you with tips and links to more advanced
    techniques to improve any AR application's development. We will introduce content
    management techniques such as multi-targets and cloud recognition, as well as
    advanced interaction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Managing your content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For computer-vision-based AR, we showed you how to build applications using
    a single target. However, there might be scenarios in which you need to use several
    markers at once. Just think of augmenting a room for which you would need at least
    one target on each wall, or you may want your application to be able to recognize
    and augment hundreds of different product packages. The former case can be achieved
    by tracking multiple targets that have a common coordinate frame, and the latter
    use case can be achieved by using the power of cloud recognition. We will briefly
    discuss both of them in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-targets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Multi-targets** are more than a collection of several individual images.
    They realize a single and consistent coordinate system where a handheld device
    can be tracked. This allows for continuous augmentation of the scene as long as
    even a single target is visible. The main challenges of creating multi-targets
    lie in defining the common coordinate system (which you will do only once) and
    maintaining the relative poses of those targets during the operation of the device.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a common coordinate system, you have to specify the translation and
    orientation of all image targets with respect to a common origin. Vuforia^(TM)
    gives you an option to even build commonly used multi-targets such as cubes or
    cuboids without getting into the details of specifying the entire target transforms.
    In the Vuforia^(TM) Target Manager, you can simply add a cube (equal length, height,
    and width) or cuboids (different length, height, and width) to a target that has
    its coordinate origin at the (invisible) center of the cuboids. All you have to
    do is to specify one extend to three extends of the cuboids and add individual
    images for all the sides of your targets, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-targets](img/8553OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you want to create more complex multi-targets, for example, for tracking
    an entire room, you have to take a slightly different approach. You first upload
    all the images you want to use for the multi-target into a single device database
    inside the Vuforia^(TM) Target Manager. After, you have downloaded the device
    database to your development machine, you can then modify the downloaded `<database
    >.xml` file to add the names of the individual image targets and their translations
    and orientations relative to the coordinate origin. A sample XML file can be found
    in the Vuforia^(TM) knowledge base under [https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file](https://developer.vuforia.com/resources/dev-guide/creating-multi-target-xml-file).
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can only have a maximum of 100 targets in your device database,
    and hence your multi-target can maximally consist of only that number of image
    targets. Also note that changing the position of image targets during the runtime
    (for example, opening a product packaging) will inhibit consistent tracking of
    your coordinate system, that is, the defined spatial relationships between the
    individual target elements would not be valid anymore. This can even lead to complete
    failure of tracking. If you want to use individual moving elements as part of
    your application, you have to define them in addition to the multi-target as separate
    image targets.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the preceding section, you can only use up to 100 images simultaneously
    in your Vuforia^(TM) application. This limitation can be overcome by using cloud
    databases. The basic idea here is that you query a cloud service with a camera
    image, and (if the target is recognized in the cloud), handle the tracking of
    the recognized target locally on your device. The major benefit of this approach
    is that you can recognize up to one million images that should be sufficient for
    most application scenarios. However, this benefit does not come for free. As the
    recognition happens in the cloud, your client has to be connected to the Internet,
    and the response time can take up to several seconds (typically around two to
    three seconds).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike, in the case of recognition, image databases stored on the device typically
    only take about 60 to 100 milliseconds. To make it easier to upload many images
    for the cloud recognition, you do not even have to use the Vuforia^(TM) online
    target manager website but can use a specific web API—the Vuforia^(TM) Web Services
    API—that can be found under the following URL: [https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api](https://developer.vuforia.com/resources/dev-guide/managing-targets-cloud-database-using-developer-api).
    You can find further information about using cloud recognition in the Vuforia^(TM)
    knowledge base by visiting [https://developer.vuforia.com/resources/dev-guide/cloud-targets](https://developer.vuforia.com/resources/dev-guide/cloud-targets).'
  prefs: []
  type: TYPE_NORMAL
- en: Improving recognition and tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to create your own natural feature-tracking targets, it is important
    to design them in a way that they can be well recognized and tracked by the AR
    system. The basics of natural feature targets were explained in the *Understanding
    natural feature tracking targets* section of [Chapter 5](ch05.html "Chapter 5. Same
    as Hollywood – Virtual on Physical Objects"), *Same as Hollywood – Virtual on
    Physical Objects*. The basic requirement for well-traceable targets is that they
    possess a high number of local features. But how do you go along if your target
    is not well recognized? To a certain extent, you can improve the tracking by using
    the forthcoming tips.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you want to make sure that your images have enough local contrast. A
    good indicator for the overall contrast in your target is to have a look at the
    histogram of its greyscale representation in any photo editing software such as
    GIMP or Photoshop. You generally want a widely distributed histogram instead of
    one with few spikes, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Improving recognition and tracking](img/8553OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To increase the local contrast in your images, you can use the photo editor
    of your choice and apply unsharpening mask filters or clarity filters, such as
    in Adobe Lightroom.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition, to avoid resampling artifacts in the Vuforia^(TM) target creation
    process, make sure to upload your individual images with an exact image width
    of 320 px. This will avoid aliasing effects and lowering the local feature count
    due to automatic server-side resizing of your images. By improving the rendering,
    Vuforia^(TM) will rescale your images to have a maximum extend of 320 px for the
    longest image side.
  prefs: []
  type: TYPE_NORMAL
- en: During the course of this book, we used different types of 3D models in our
    sample applications, including basic primitives (such as our colored cube or sphere)
    or more advanced 3D models (such as the ninja model). For all of them, we didn't
    really consider the realistic aspect, including the light condition. Any desktop
    or mobile 3D application will always consider how the rendering looks realistic.
    This photorealistic quest always passes through the quality of the geometry of
    the model, the definition of their appearance (material reflectance properties),
    and how they interact with light (shading and illumination).
  prefs: []
  type: TYPE_NORMAL
- en: '**Photorealistic rendering** will expose properties such as occlusion (what
    is in front of, behind something), shadows (from the illumination), support for
    a range of realistic material (developed with shader technology), or more advanced
    properties such as supporting global illumination.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you develop your AR applications, you should also consider photorealistic
    rendering. However, things are a bit more complicated because in AR, you not only
    consider the virtual aspect (for example, a desktop 3D game) but also the real
    aspect. Supporting photorealism in AR will imply that you consider **how real
    (R**) **environments and virtual** (**V**) **environments** also interact during
    the rendering that can be simplified as follows through four different cases:'
  prefs: []
  type: TYPE_NORMAL
- en: V→V
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: V→R
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R→V
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R→R
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The easiest thing you can do is support V→V, which means that you enable any
    of the advanced rendering techniques in your 3D rendering engine. For computer-vision-based
    applications, it will mean that everything looks realistic on your target. For
    sensor-based applications, it will mean that your virtual object seems realistic
    between each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second easy step, especially for computer-vision-based applications, is to
    support V→R using a plane technique. If you have a target, you can create a semi-transparent
    version of it and add it to your virtual scene. If you have shadows enabled, it
    will seem that the shadow is projecting on to your target, creating a simple illusion
    of V→R. You can refer to the following paper which will provide you with some
    technical solutions to this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *A real-time shadow approach for an augmented reality application
    using shadow volumes. VRST 2003: 56-65* by *Michael Haller*, *Stephan Drab*, and
    *Werner Hartmann*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling R→V is a bit more complicated and still a difficult research topic.
    For example, support illumination of virtual objects by physical light sources
    requires a lot of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, occlusion is easy to implement for R→V. Occlusion in the case of R→V
    can happen if, for example, a physical object (such as a **can**) is placed in
    front of your virtual object. In standard AR, you always render the virtual content
    in front of the video, so your **can** will appear to be behind even though it
    can be in front of your target.
  prefs: []
  type: TYPE_NORMAL
- en: A simple technique to reproduce this effect is sometimes referred to as **phantom
    object**. You need to create a virtual counterpart of your physical object, such
    as a cylinder, to represent your can. Place this virtual counterpart at the same
    position as the physical one and do a **depth-only rendering**. Depth-only rendering
    is available in a large range of libraries, and it's related to the color mask
    where, when you render anything, you can decide which channel to render. Commonly,
    you have the combination of red, green, blue, and depth. So, you need to deactivate
    the first three channels and only activate depth. It will render some sort of
    phantom object (no color but only depth), and via the standard rendering pipeline,
    the video will not be occluded anymore where you have your real object, and occlusion
    will look realistic; see, for example, [http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf](http://hal.inria.fr/docs/00/53/75/15/PDF/occlusionCollaborative.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: This is the simple case; when you have a dynamic object, things are way more
    complicated, and you need to be able to track your objects, to update their phantom
    models, and to be able to get a photorealistic rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced interaction techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapter, we looked at some simple interaction techniques, that
    included ray picking (via touch interaction), sensor interaction, or camera to
    target proximity. There are a large number of other interaction techniques that
    can be used in Augmented Reality.
  prefs: []
  type: TYPE_NORMAL
- en: One standard technique that we will also find on other mobile user interfaces,
    is a **virtual control pad**. As a mobile phone limits access to additional control
    devices, such as a joypad or joystick, you can emulate their behavior via a touch
    interface. With this technique, you can display a virtual controller on your screen
    and analyze the touch in this area as being equivalent to controlling a control
    pad. It's easy to implement and enhance the basic ray-casting technique. Control
    pads are generally displayed near the border of the screen, adapting to the form
    factor and grasping the gesture you make when you hold the device, so you can
    hold the device with your hand and naturally move your finger on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique that is really popular in Augmented Reality is **Tangible
    User Interface** (**TUI**). When we created the sample using the concept of a
    camera to target proximity, we practically implemented a Tangible User Interface.
    The idea of a TUI is to use a physical object for supporting interaction. The
    concept was largely developed and enriched by *Iroshi Ishii* from the Tangible
    Media Group at MIT—the website to refer to is [http://tangible.media.mit.edu/](http://tangible.media.mit.edu/).
    *Mark Billinghurst* during his Ph.D. applied this concept to Augmented Reality
    and demonstrated a range of dedicated interaction techniques with it.
  prefs: []
  type: TYPE_NORMAL
- en: The first type of TUI AR is **local interaction**, where you can, for example,
    use two targets for interaction. Similar to the way we detected the distance between
    the camera and target in our `ProximityBasedJME` project, you can replicate the
    same idea with two targets. You can detect whether two targets are close to each
    other, aligned in the same direction, and trigger some actions with it. You can
    use this type of interaction for card-based games when you want cards to interact
    with each other, or games that include puzzles where users need to combine different
    cards together, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second type of TUI AR is **global interaction** where you will also use two
    or more targets, but one of the targets will become *special*. What you do in
    this case is define a target as being a base target, and all the other targets
    refer to it. To implement it, you just compute the local transformation of the
    other targets to the base target, with the base target behind and defined as your
    origin. With this, it''s really easy to place targets on the main target, somehow
    defining some kind of ground plane and performing a range of different types of
    interaction with it. *Mark Billinghurst* introduced a famous derivate version
    of it, for performing paddle-based interaction. In this case, one of the targets
    is used as a paddle and can be used to interact on the ground plane—you can touch
    the ground plane, have the paddle at a specific position on the ground plane,
    or even detect a simple gesture with it (shake the paddle, tilt the paddle, and
    so on). To set up mobile AR, you need to consider the fact that end users hold
    a device and can''t perform complex gestures, but with a mobile phone, interaction
    with one hand is still possible. Refer to the following technical papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tangible augmented reality. ACM SIGGRAPH ASIA (2008): 1-10* by *Mark Billinghurst*,
    *Hirokazu Kato*, and *Ivan Poupyrev*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Designing augmented reality interfaces. ACM Siggraph Computer Graphics 39.1
    (2005): 17-22* by *Mark Billinghurst*, *Raphael Grasset*, and *Julian Looser*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Global interaction with a TUI, in a sense, can be defined as interaction *behind
    the screen*, while virtual control pad can be seen as interaction *in front of
    the screen*. This is another way to classify interaction with a mobile, which
    brings us to the third category of interaction techniques: **touch interaction
    on the target**. The Vuforia^(TM) library implements, for example, the concept
    of virtual buttons. A specific area on your target can be used to place the controller
    (for example, buttons, sliders, and dial), and users can place their finger on
    this area and control these elements. The concept behind this uses a time-based
    approach; if you keep your finger placed on this area for a long time, it simulates
    a click that you can have on a computer, or a tap you can do on a touch screen.
    Refer to [https://developer.vuforia.com/resources/sample-apps/virtual-button-sample-app](https://developer.vuforia.com/resources/sample-apps/virtual-button-sample-app),
    for example.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other techniques that are investigated in research laboratories, and
    they will soon become available to the future generation of mobile AR, so you
    should already think about them also when will be available. One trend is towards
    3D gesture interaction or also called **mid-air interaction**. Rather than touching
    your screen or touching your target, you can imagine making gestures between the
    device and the target. Having a mobile AR for 3D modeling would be an appropriate
    technique. 3D gestures have a lot of challenges such as recognizing the hand,
    the fingers, the gesture, physical engagement that can result in fatigue, and
    so on. In the near future, this type of interaction, which is already popular
    on smart home devices (such as Microsoft Kinect), will be available on devices
    (equipped with 3D sensors).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed you how to go beyond the standard AR applications
    by using multi-targets or cloud recognition for computer-vision-based AR. We also
    showed you how you can improve the tracking performance for your image targets.
    In addition, we introduced you to some advanced rendering techniques for your
    AR applications. Finally, we also showed you some novel interaction techniques
    that you can use to create great AR experiences. This chapter concludes your introduction
    to the world of Augmented Reality development for Android. We hope you are ready
    to progress onto new levels of AR application development.
  prefs: []
  type: TYPE_NORMAL
